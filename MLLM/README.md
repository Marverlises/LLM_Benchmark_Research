# 《MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models》

作者为Chaoyou Fu等人，来自腾讯优图实验室和厦门大学。

### 1. 引言与背景


文章首先介绍了多模态大语言模型（MLLM）的兴起。MLLM以大语言模型（LLM）为核心，结合视觉、文本等多种模态输入，能够处理复杂的多模态任务，例如基于图像生成诗歌或进行复杂推理。这些模型继承了LLM的三大能力：**指令遵循**（Instruction Following）、**上下文学习**（In-Context Learning, ICL）和**思维链推理**（Chain-of-Thought, CoT）。例如，Flamingo支持多模态上下文学习，PaLM-E实现无OCR的数学推理，GPT-4V在复杂推理任务中表现出色，MiniGPT-4则通过多模态指令调整实现了图像到网站代码的转换。


然而，现有的评估方式存在局限性：


1. **传统多模态数据集评估**（如图像描述、视觉问答VQA）：难以反映MLLM的新兴能力，且由于训练数据的不统一，存在测试集泄露风险。
2. **开放式评估**：数据量有限（如仅50张图像）或数据未公开。
3. **单一能力评估**：如仅关注物体幻觉（Object Hallucination）或对抗鲁棒性，难以全面评估模型性能。

因此，文章提出MME基准，旨在提供一个综合、公平、量化的评估框架，满足以下四个特点：


1. **全面性**：涵盖感知（Perception）和认知（Cognition）能力，包含14个子任务。
2. **避免数据泄露**：所有指令-答案对均为手动设计，尽量不直接使用公开数据集的标注。
3. **简洁指令**：指令设计简洁统一，避免提示工程（Prompt Engineering）的影响，确保公平比较。
4. **便于量化**：通过“是/否”回答形式，便于准确、客观的量化统计。


### 2. MME评估基准的设计


MME基准的设计包括指令设计、评估指标和数据收集三个主要部分，以下逐一解析：


#### 2.1 指令设计


为了便于量化统计，MME的指令设计以“是/否”回答为核心。每张测试图像对应两条指令，分别为一个“是”答案和一个“否”答案。例如，“图中是否有猫？请回答是或否”和“图中是否有狗？请回答是或否”。这种设计确保模型不仅能正确识别图像内容，还能通过不同问题验证其理解的准确性，减少猜测的可能性。


指令由两部分组成：


- **简洁问题**：直接明了，符合人类认知习惯。
- **固定后缀**：“请回答是或否”，确保模型输出统一，便于评估。

#### 2.2 评估指标


MME采用两种评估指标：


- **Accuracy**：基于单条指令的正确率，随机正确率为50%。
- **Accuracy+**：基于每张图像两条指令均正确的情况，随机正确率为25%。此指标更严格，反映模型对图像的综合理解能力。
- **子任务得分**：Accuracy和Accuracy+之和，每个子任务满分为200。
- **感知与认知总分**：感知任务（10个子任务）总分2000，认知任务（4个子任务）总分800。

#### 2.3 数据收集


MME涵盖14个子任务，分为感知和认知两大类，所有指令-答案对均为手动设计，部分图像来自公开数据集（如COCO），但不直接使用其标注，以避免数据泄露。数据来源包括真实拍摄照片和生成图像。


##### 2.3.1 感知任务（10个子任务）


感知任务评估模型识别图像内容的能力，分为粗粒度识别、细粒度识别和OCR：


1. **粗粒度识别**（Coarse-Grained Recognition，4个子任务）：

- **存在性**（Existence）：识别图像中是否存在特定物体（如“图中有猫吗？”）。
- **计数**（Count）：识别物体数量。
- **位置**（Position）：识别物体位置。
- **颜色**（Color）：识别物体颜色。
- 数据：每子任务包含30张COCO图像，共60条指令-答案对。

2. **细粒度识别**（Fine-Grained Recognition，5个子任务）：

- **电影海报**（Poster，147张图像）：识别特定电影海报。
- **名人**（Celebrity，170张图像）：识别红框中人物是否为指定名人。
- **场景**（Scene，200张图像）：识别特定场景。
- **地标**（Landmark，200张图像）：识别特定地标。
- **艺术品**（Artwork，200张图像）：识别特定艺术品。
- 数据来源：公开数据集（如MovieNet、Places等），但指令手动设计。

3. **光学字符识别**（OCR，1个子任务）：

- 识别图像中的文字，包含20张图像和40条指令-答案对，样本较简单。

##### 2.3.2 认知任务（4个子任务）


认知任务评估模型在感知基础上进行逻辑推理的能力，要求结合图像内容和语言模型的知识：


1. **常识推理**（Commonsense Reasoning，70张图像，140条指令-答案对）：

- 测试日常生活中基本知识的判断，如“冷天适合穿羽绒服吗？”。
- 数据：手动拍摄或通过扩散模型生成。

2. **数值计算**（Numerical Calculation，20张图像，40条指令-答案对）：

- 要求从图像中读取算术问题并直接输出答案，如简单的加法或乘法。

3. **文本翻译**（Text Translation，时间20张图像，40条指令-答案对）：

- 将图像中的中文翻译成英文，当前版本设计简单翻译问题。

4. **代码推理**（Code Reasoning，20张图像，40条指令-答案对）：

- 读取图像中的代码并完成逻辑运算，如基础代码问题。


### 3. 实验与结果


文章对30个先进的MLLM进行了零样本（Zero-Shot）性能评估，包括BLIP-2、InstructBLIP、MiniGPT-4、GPT-4V、Qwen-VL-Chat等。实验结果通过感知和认知总排行榜（Fig. 2）以及14个子任务的排行榜展示。


#### 3.1 感知能力结果


感知任务包含10个子任务，评估粗粒度识别、细粒度识别和OCR能力：


- **粗粒度识别**：

**存在性**：Otter、Lynx、WeMM、Muffin、SPHINX得分最高（195分，Accuracy 98.33%，Accuracy+ 96.67%）。
**计数**：Muffin表现最佳。
**位置**：Lion和SPHINX并列第一，但整体表现最差，表明模型对位置信息不敏感。
**颜色**：InfMLLM排名第一。

- **细粒度识别**：

**电影海报**：GPT-4V、Lion、Qwen-VL-Chat位列前三。
**名人**：WeMM、SPHINX、Otter领先，GPT-4V因拒绝回答涉及个人的问题得分0。
**场景**：WeMM、InfMLLM、Lynx排名前三。
**地标**：Lion、WeMM、LLaVA位列前三。
**艺术品**：WeMM、GPT-4V、GIT2领先，但GPT-4V因拒绝回答部分私人艺术品问题得分较低。

- **OCR**：GPT-4V以185分大幅领先，Skywork-MM和WeMM分列二、三位。
- **感知总分**：WeMM、InfMLLM、SPHINX位列前三，Lion、LLaVA、XComposer-VL紧随其后。

#### 3.2 认知能力结果


认知任务包含4个子任务，评估逻辑推理能力：


- **常识推理**：GPT-4V得分最高（142.14分），WeMM和XComposer-VL次之。
- **数值计算**：GPT-4V仍然第一。
- **文本翻译**：GPT-4V表现较弱，其他模型得分均未超过150，表明认知任务仍有较大改进空间。
- **代码推理**：GPT-4V以170分遥遥领先。
- **认知总分**：GPT-4V、Lion、WeMM位列前三。


### 4. 分析与发现


实验揭示了MLLM的四个常见问题，为后续优化提供指导：


1. **不遵循指令**：尽管指令设计简洁，部分模型未按“是/否”回答，而是自由表达，导致错误。例如，模型可能仅陈述事实而不明确回答“是”或“否”。
2. **感知能力不足**：模型在物体计数或文字识别中易出错，且对指令细微变化敏感，导致矛盾结果。
3. **推理能力不足**：即使模型正确感知图像内容，也可能因逻辑链断裂给出错误答案。例如，模型知道图像不是办公室却回答“是”。
4. **物体幻觉**：当指令提及图像中不存在的物体时，模型可能“想象”其存在，导致错误回答“是”，影响Accuracy+得分。

这些问题表明，当前MLLM在指令遵循、感知准确性、推理逻辑和幻觉抑制方面仍有改进空间。文章建议通过加入CoT提示（如“逐步思考”）可能提升推理性能。

### 5. 结论与贡献


MME是首个综合评估MLLM的基准，具有以下贡献：


1. **提出MME基准**：覆盖感知和认知能力，包含14个子任务，满足全面性、公平性和量化需求。
2. **评估30个MLLM**：揭示模型在不同任务中的表现差异，表明仍有较大改进空间。
3. **总结问题**：识别指令遵循、感知、推理和幻觉等问题，为模型优化提供方向。

MME的数据和排行榜已公开，网址为：[https://github.com/BradyFU/Awesome-MultimodalLarge-Language-Models/tree/Evaluation。](https://github.com/BradyFU/Awesome-MultimodalLarge-Language-Models/tree/Evaluation%E3%80%82)

# 《MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities》

### **1. 论文概述**


论文标题为《MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities》，由 Weihao Yu 等人在 2024 年发表，会议为第 41 届国际机器学习大会（ICML 2024）。该论文提出了一种新的评估基准 **MM-Vet**（Multimodal Veterinarian），用于评估大型多模态模型（Large Multimodal Models, LMMs）在复杂多模态任务上的综合能力。


#### **研究背景**


随着大型语言模型（LLMs）在自然语言处理领域的突破（如 GPT-4、LLaMA），研究者开始探索将视觉能力融入语言模型，发展出大型多模态模型（LMMs）。这些模型能够处理视觉和语言输入，解决复杂任务，如识别黑板上的数学问题、理解新闻图片中的事件或解释视觉笑话。然而，现有的视觉-语言（Vision-Language, VL）基准测试（如 VQA、COCO）通常只关注单一或少量能力的评估，无法充分测试 LMMs 的综合能力。因此，论文提出 MM-Vet，以系统性地评估 LMMs 在多种核心视觉-语言能力的整合表现。


#### **研究目标与挑战**


论文旨在解决以下三个主要挑战：


1. **如何系统化地结构化和评估复杂多模态任务**：复杂任务通常需要多种能力的整合，现有基准缺乏对这些整合能力的系统性测试。
2. **如何设计适用于不同问题和回答类型的评估指标**：多模态任务的回答形式多样（如单字答案、长篇文本），需要统一的评估方法。
3. **如何提供超越简单性能排名的模型洞察**：单一的排名可能因数据集构成而产生偏差，需提供更细粒度的能力分析。


### **2. MM-Vet 基准的核心设计**


MM-Vet 的设计基于一个核心洞察：LMMs 解决复杂多模态任务的能力来源于其对多种核心视觉-语言能力的整合。论文定义了 **6 种核心视觉-语言能力**，并基于这些能力的组合衍生出 **16 种整合任务**，用于评估 LMMs 的综合性能。


#### **2.1 六种核心视觉-语言能力**


论文定义了以下六种核心能力，每种能力对应现实场景中的典型需求（详见文档第 3 页）：


1. **识别（Recognition, Rec）**：包括场景、物体、物体属性（如颜色、形状）识别及计数等高级视觉识别任务。
2. **知识（Knowledge, Know）**：涵盖社会常识、视觉常识、百科知识及新闻等时间敏感知识，需模型不仅拥有知识，还能有效应用。
3. **光学字符识别（OCR）**：测试模型读取图像中的文本并基于文本进行推理的能力。
4. **空间意识（Spatial Awareness, Spat）**：理解物体和场景文本之间的空间关系。
5. **语言生成（Language Generation, Gen）**：要求模型生成清晰、引人入胜且信息丰富的回答，特别针对需要长篇回答的问题。
6. **数学（Math）**：评估模型在现实场景中解决数学方程或算术问题的能力。

这些能力通过现实场景的例子进行说明，例如：


- 识别智能手机（识别能力）并查看时间（OCR 能力）。
- 根据空间线索（如“位于体育场对面”）找到商店（空间意识）。
- 计算折扣后牛奶的价格（数学能力）。

#### **2.2 数据集构建**


MM-Vet 数据集包含 **187 张图片和 205 个问题**，每个问题需要一种或多种核心能力来回答。数据集设计特点如下：


- **问题多样性**：问题类型多样，包括开放式短回答（如“有多少个西红柿？”）和长篇回答（如总结图表信息）。
- **高质量标注**：155 个问题的答案由作者精心标注，50 个答案从网络收集，确保数据质量。
- **额外数据**：从 VCR 数据集中选取 10 张高质量图片，补充数据集。
- **能力整合**：大多数样本需要多种能力整合，例如解释视觉笑话需要识别、知识和语言生成能力。

文档第 3 页的图 2 显示了各能力及能力整合的比例，表明大多数样本涉及多种能力（比例总和超过 100%），强调了 MM-Vet 对综合能力的关注。


#### **2.3 评估指标：基于 LLM 的开放式评估器**


传统评估指标难以应对多模态任务的多样性（如短答案 vs 长篇文本）。为此，论文提出了一种 **基于 LLM 的评估器**，使用 GPT-4 进行评分，特点如下：


- **统一评分标准**：评分范围为 0 到 1，适用于不同问题类型和回答风格。
- **少样本提示（Few-shot Prompting）**：通过提供五种短回答和两种长回答的示例，定义评分标准，GPT-4 自动推断评分规则（见文档第 4 页表 1）。
- **灵活性**：支持未来扩展到新问题类型（如边界框定位），无需手动定义答案风格。
- **评分公式**：

总得分： $S = \frac{\sum_{i=1}^N s_i}{N} \times 100\%$，其中 $s_i$ 为样本 $i$ 的得分，$N$ 为样本总数。
能力/整合得分： $S_c = \frac{\sum s_i}{N_c} \times 100\%$，其中 $C$ 为特定能力或整合的样本集。

为减少 GPT-4 输出的随机性，论文对每个样本重复评估 5 次，取平均值。



### **3. 实验与结果分析**


MM-Vet 评估了两类 LMM 系统：**端到端训练的 LMMs**（如 OpenFlamingo、LLaVA、MiniGPT-4、InstructBLIP）和 **使用工具的 LLM 系统**（如 MM-ReAct、Transformers Agent）。此外，还评估了闭源商业模型（如 GPT-4V 和 Bard），但将其结果单独标记以避免不公平比较。


#### **3.1 实验设置**


- **评估模型**：包括 OpenFlamingo、BLIP-2、LLaVA、MiniGPT-4、LLaMA-Adapter V2、Otter、InstructBLIP、MM-ReAct（基于 GPT-3.5 和 GPT-4）、Transformers Agent 以及 Bard 和 GPT-4V。
- **评估流程**：对每个样本，填充问题、模型输出和标准答案到 GPT-4 的提示模板，生成 0 到 1 的得分，重复 5 次取平均值。
- **结果展示**：按核心能力和能力整合分别报告平均得分，并分析模型在不同能力上的表现。

#### **3.2 主要结果（文档第 5 页表 2 和表 9）**


- **核心能力表现**：

**LLaVA-13B (V1.3, 336px)** 在识别（38.1%）、OCR（22.3%）、知识（25.2%）、语言生成（25.2%）和空间意识（25.3%）上表现优异，总得分为 31.2%。
**MM-ReAct-GPT-4** 在 OCR（66.3%）、空间意识（60.6%）和数学（72.0%）上表现突出，表明外部工具的集成能显著提升特定能力。
**Bard** 在 Bard 子集（剔除了含人脸的 168 个样本）上总得分最高（53.5%），在识别（56.2%）、知识（50.9%）和语言生成（61.0%）上领先。

- **能力整合表现**（文档第 14 页表 10）：

Bard 在 7/15 的能力整合任务中排名第一，MM-ReAct-GPT-4 在 9/15 的任务中领先，特别是在需要 OCR 和数学的整合任务中表现更好。
端到端模型（如 LLaVA）与闭源模型相比仍有差距，尤其在复杂整合任务上。

#### **3.3 GPT-4V 成功与失败案例（文档第 22-23 页）**


- **成功案例**：

正确计算汽油价格（$50 可购买 13.7 加仑，接近真实值 13.5）。
准确识别 Tina Fey 的国际知名度（35%）及其最高知名度国家（澳大利亚）。
正确识别包装类别在塑料回收中的表现最差。

- **失败案例**：

在停车位 33 的车辆识别中错误（预测为蓝色车，实际为空）。
2023 年平均固定费用和总成本预测错误（分别为 £280 和 £5,890，实际为 £271 和 £23,800）。

这些案例显示 GPT-4V 在 OCR 和知识任务上表现较好，但在复杂空间推理和精确数学计算上仍有不足。


#### **3.4 模型洞察**


- **端到端 vs 工具使用系统**：

端到端模型（如 LLaVA）在识别和语言生成上表现较好，但在 OCR 和数学任务上较弱。
工具使用系统（如 MM-ReAct）通过调用外部工具（如 OCR 或计算器）显著提升了特定能力的性能，尤其在数学和空间意识任务上。

- **开源 vs 闭源模型**：

闭源模型（如 Bard、GPT-4V）整体性能优于开源模型，特别是在综合能力整合任务上。
开源模型（如 LLaVA）在训练数据、视觉编码器和 LLM 选择上存在差异，影响其在不同能力上的表现。

- **影响因素**：

**训练数据**：多样化的训练数据（如包含 OCR 和数学任务的数据）有助于提升模型性能。
**视觉编码器**：更高分辨率的视觉输入（如 LLaVA-13B 的 336px）提升了识别和空间意识能力。
**LLM 选择**：更强大的 LLM（如 LLaMA-2）能提升语言生成和知识相关任务的表现。


### **4. 主要贡献**


论文的贡献可以总结为以下三点：


1. **提出 MM-Vet 基准**：设计了一个包含 16 种能力整合任务的评估框架，基于 6 种核心视觉-语言能力，填补了复杂多模态任务评估的空白。
2. **开发 LLM 评估器**：提出基于 GPT-4 的开放式评估方法，统一了不同问题类型和回答风格的评分标准，具有扩展性和灵活性。
3. **提供模型洞察**：通过对多种 LMM 的评估，揭示了不同系统范式（端到端 vs 工具使用）和模型设计的优劣势，为未来 LMM 开发提供指导。


### **5. 相关工作与 MM-Vet 的差异**


论文在第 3 页讨论了相关工作，指出 MM-Vet 与现有视觉-语言基准（如 VQA、COCO、TextCaps）和并发评估研究（如 MME、MMBench）的区别：


- **传统基准**：如 VQA（Goyal et al., 2017）关注单一能力（如视觉识别或图像描述），无法测试复杂任务的整合能力。
- **并发研究**：如 MME 和 MMBench 提供了综合样本，但缺乏对能力整合的明确定义和分析。MM-Vet 明确定义了 6 种核心能力和 16 种整合任务，提供更细粒度的能力洞察。
- **LLM 评估**：MM-Vet 借鉴了 NLP 领域的 LLM 评估方法（如 Chiang & Lee, 2023），并将其扩展到多模态任务，统一了评分标准。


### **6. 实际意义与未来方向**


- **实际意义**：

MM-Vet 提供了一个系统化的框架，帮助研究者理解 LMMs 的综合能力，识别其在特定任务上的优势和不足。
基于 LLM 的评估器为开放式回答提供了灵活的评分方法，适用于未来多模态任务的扩展。
对开源和闭源模型的比较为模型设计提供了启示，例如工具使用系统在特定任务上的潜力。

- **未来方向**：

扩展数据集规模，覆盖更多复杂任务和能力整合。
优化评估器的鲁棒性，减少 GPT-4 评分的随机性。
探索更多开源模型的训练策略，以缩小与闭源模型的差距。

- 代码和数据（[https://github.com/yuweihao/MM-Vet）以及在线评估器（https://huggingface.co/spaces/whyu/MM-Vet_Evaluator）

# 《MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models for Integrated Capabilities》

作者包括 Weihao Yu 等，来自新加坡国立大学、微软和先进微设备公司（AMD）。

### 一、背景与引言


大型多模态模型（Large Multimodal Models, LMMs）近年来发展迅速，展现出解决复杂任务的强大能力，例如图形用户界面（GUI）导航、截图到代码转换、视频理解等。这些任务通常需要多种能力的综合运用，包括视觉识别、语言生成、空间感知、知识推理、OCR（光学字符识别）和数学计算等。然而，现有的评估基准（如 MM-Vet）主要针对单幅图像-文本对的评估，难以反映现实世界中常见的图像-文本序列处理场景，如多帧视频分析或图像比较任务。


为解决这一问题，文章提出了 **MM-Vet v2**，一个扩展的评估基准，旨在更全面地评估 LMMs 的综合能力。MM-Vet v2 在原 MM-Vet 的基础上引入了新的核心能力——**图像-文本序列理解（image-text sequence understanding）**，并通过增加高质量评估样本的数量，提升了评估的全面性和挑战性。



### 二、MM-Vet v2 的核心贡献


文章的贡献主要体现在以下几个方面：


1. **新增核心能力：图像-文本序列理解**

- MM-Vet v2 在 MM-Vet 定义的六种核心能力（识别、知识、空间感知、语言生成、OCR、数学）基础上，新增了“图像-文本序列理解”能力。这一能力评估模型处理交错图像-文本序列的能力，模拟现实场景中的复杂任务。例如：

**视频帧序列理解**：如图 1(c) 所示，模型需要分析多帧图像以回答关于时间序列的问题。
**图像比较任务**：如图 1(d) 所示，模型需要比较两幅图像以找出差异。

- 这种能力被认为是迈向更强通用智能的重要一步，因为它要求模型在处理多模态数据时能够综合理解上下文和序列关系。

2. **扩展高质量评估集**

- 原 MM-Vet 只有 217 个评估样本，受限于高质量数据收集的难度。MM-Vet v2 扩展到 **517 个问题**，覆盖从日常生活到专业/行业应用的多种场景。
- 数据收集分为两步：

**问题生成**：研究人员（而非众包工作者）设计复杂且有意义的问题，确保覆盖多样化场景。这些问题基于对 GPT-4V 的探索性报告扩展而来。
**答案生成**：对于简单问题，专家直接标注答案；对于需要长段落回答的复杂问题，先使用 GPT-4V 草拟答案，再由专家校对和改写，确保答案的准确性和高质量。

3. **改进的评估方法**

- MM-Vet v2 沿用了 MM-Vet 的评估框架，但改进了提示设计以支持图像-文本序列。例如，在提示中加入 &lt;image&gt; 标记图像位置，明确了序列中的图像和文本关系。
- 评估使用 GPT-4（具体为 gpt-4-0613 模型）对模型输出与标准答案进行比较，基于 &lt;AND&gt; 和 &lt;OR&gt; 逻辑判断正确性：

&lt;AND&gt;：预测需包含标准答案中的所有元素才算完全正确。
&lt;OR&gt;：预测包含标准答案中的任一元素即算完全正确。

4. **开源资源**

- 文章提供了代码、数据和在线评估工具：

代码和数据：[https://github.com/yuweihao/MM-Vet](https://github.com/yuweihao/MM-Vet)
在线评估器：[https://huggingface.co/spaces/whyu/MM-Vet-v2_Evaluator](https://huggingface.co/spaces/whyu/MM-Vet-v2_Evaluator)
排行榜：[https://paperswithcode.com/sota/visual-question-answering-on-mm-vet-v2](https://paperswithcode.com/sota/visual-question-answering-on-mm-vet-v2)


### 三、MM-Vet v2 的数据集与评估器


MM-Vet v2 的设计目标是构建一个高质量的评估集，专注于以下七种核心能力：


1. **识别（Recognition, Rec）**：识别图像中的对象、场景或事件。
2. **知识（Knowledge, Know）**：利用外部知识回答问题。
3. **空间感知（Spatial Awareness, Spat）**：理解图像中的空间关系。
4. **语言生成（Language Generation, Gen）**：生成连贯且准确的文本回答。
5. **OCR**：从图像中提取和理解文本。
6. **数学（Math）**：解决数学相关问题。
7. **图像-文本序列理解（Seq）**：处理交错的图像和文本序列。

与 MM-Vet 不同，MM-Vet v2 的问题格式支持多图像-文本序列，显著提高了评估的复杂性和现实性。图 2 展示了各核心能力及其组合的比例，表明 MM-Vet v2 在能力覆盖上更加均衡。



### 四、实验结果


MM-Vet v2 对多种先进的 LMMs 进行了评估，结果在表 2（核心能力）和表 3（能力组合）中展示。以下是关键结果的详细分析：


1. **总体性能**

- **Claude 3.5 Sonnet** 以 71.8 的得分位居榜首，略高于 **GPT-4o** 的 71.0。
- 开放权重模型中，**InternVL2-Llama3-76B** 表现突出，得分为 68.4，接近顶级闭源模型。
- 其他模型如 GPT-4V、Gemini 1.5 Pro、Qwen-VL-Max 等得分在 50.8 左右，表现较为接近。

2. **核心能力表现（表 2）**

- **Claude 3.5 Sonnet** 在识别（69.2%）、语言生成（70.8%）、OCR（70.8%）、空间感知（50.8%）和知识（53.4%）上表现优异。
- **GPT-4o** 在图像-文本序列理解（53.3%）和数学（11.8%）上略胜一筹。
- **InternVL2-Llama3-76B** 在多个能力上表现强劲，尤其在识别（67.0%）、语言生成（68.5%）、OCR（65.6%）和序列理解（65.4%）上接近或超过闭源模型。
- 数学能力（Math）是所有模型的弱项，最高得分仅为 11.8%，表明 LMMs 在数学推理方面仍有较大提升空间。

3. **能力组合表现（表 3）**

- 表 3 展示了 16 种最常见的能力组合（如 Rec+Gen+Know、Rec+OCR 等）的评估结果。
- Claude 3.5 Sonnet 和 GPT-4o 在大多数组合上表现优异，尤其是在需要多种能力协同的任务中。
- InternVL2-Llama3-76B 在开放权重模型中表现突出，特别是在涉及序列理解的组合任务中。


### 五、结论与意义


MM-Vet v2 通过引入图像-文本序列理解能力和扩展高质量评估集，显著提升了 LMMs 综合能力的评估标准。其主要意义包括：


1. **更贴近现实世界的评估**：图像-文本序列理解能力反映了现实场景中的复杂多模态任务，如视频分析和多图像比较。
2. **高质量数据与严格评估**：通过专家设计问题和校对答案，MM-Vet v2 确保了评估的准确性和可靠性。
3. **推动模型发展**：评估结果显示，即使顶级模型（如 Claude 3.5 Sonnet 和 GPT-4o）在某些能力（如数学）上仍有不足，为未来研究指明了方向。


### 六、局限性与未来工作


文章未明确讨论局限性，但从内容推测，可能的局限性包括：


- **数学能力评估不足**：数学得分普遍较低，可能是因为数据集中的数学问题较少或复杂度不足。
- **序列理解任务的多样性**：尽管引入了序列理解能力，但样本可能未完全覆盖所有可能的序列任务（如更复杂的动态视频理解）。
- **评估成本**：使用 GPT-4 作为评估器可能增加成本，且依赖单一模型可能引入偏差。

未来工作可能包括：


- 进一步扩展数据集，增加数学和序列理解任务的多样性。
- 开发更高效的评估方法，减少对外部模型（如 GPT-4）的依赖。
- 探索更复杂的多模态任务，如实时交互或跨模态推理。


### 七、总结

《MM-Vet v2》是一项重要的多模态模型评估基准，通过引入图像-文本序列理解能力和扩展高质量评估集，显著提高了评估的挑战性和现实性。实验结果显示 Claude 3.5 Sonnet 和 GPT-4o 是当前最强的模型，而 InternVL2-Llama3-76B 在开放权重模型中表现突出。该基准为 LMMs 的发展提供了重要参考，同时也指出了数学推理等领域的改进空间。

# 《MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?》

由Yi-Fan Zhang等人撰写，发表于2024年。本文提出了一种新的多模态大型语言模型（MLLM）评估基准MME-RealWorld，旨在解决现有基准在评估MLLM在高分辨率、现实世界复杂场景中的能力时存在的问题。

### **1. 背景与研究动机**


近年来，多模态大型语言模型（MLLMs）得到了快速发展，这些模型通过整合视觉、文本等多种模态数据，旨在成为能够全面感知人类查询和环境情况的通用智能体。然而，现有的MLLM评估基准在以下几个方面存在局限性，难以准确衡量模型在现实世界中的表现：


1. **数据规模不足**：许多现有基准（如MME、MMBench、MM-Vet等）包含的问答（QA）对数量较少，通常少于1万对。这种小规模数据导致评估结果波动较大，难以可靠地比较模型性能。
2. **数据质量受限**：一些较大规模的基准（如MMT-Bench、SEEDBench）依赖LLM或MLLM自动生成标注，而这些模型自身的性能限制会引入噪声，影响标注质量。例如，文章提到即使最好的模型（如InternVL-2）在MME-RealWorld上的准确率仅为50%，依赖模型生成标注可能导致质量下降。
3. **任务难度不足**：现有基准的顶尖模型性能已达到80%-90%，先进模型之间的性能差距较小，难以区分模型的能力。此外，现有基准的任务通常较为简单，缺乏对高分辨率图像和复杂现实场景的挑战。
4. **图像分辨率限制**：现有基准的图像分辨率较低（例如MME的平均分辨率为1161×840像素），无法充分体现现实世界中高分辨率图像的细节需求，如体育比赛中的记分牌或遥感图像中的小型物体。

为了应对这些问题，作者提出了MME-RealWorld基准，专注于高分辨率图像和现实世界中的复杂场景，旨在为MLLM提供更具挑战性的评估环境。

### **2. MME-RealWorld基准概述**


MME-RealWorld是一个大规模、高分辨率、人工标注的多模态评估基准，包含以下核心特点：


- **数据规模**：收集了超过30万张图像，从中筛选出13,366张高分辨率图像，平均分辨率为2000×1500像素，最高分辨率达到5304×7952像素（约4217万像素）。这些图像生成了29,429个问答对，覆盖5个现实世界场景和43个子任务。这是目前已知的最大规模人工标注多模态基准。
- **数据质量**：所有问答对均由25名专业标注者和7名MLLM专家手动完成，并经过多轮交叉检查，确保高质量和准确性。图像分辨率远超现有基准，适合测试模型对细节的感知能力。
- **任务难度**：任务设计复杂，涵盖感知和推理两个层面，涉及现实世界的挑战性场景，如在监控视频中计数133辆车，或在高分辨率遥感图像中识别小型物体。实验结果显示，即使最先进的模型在该基准上的准确率也未超过60%，凸显其高难度。
- **现实应用性**：聚焦于5个现实世界场景：野外光学字符识别（OCR）、遥感（RS）、图表与表格（DT）、自动驾驶（AD）、监控（MO）。这些场景对人类而言也具有挑战性，反映了MLLM在实际应用中的潜在价值。
- **中文版本（MME-RealWorld-CN）**：针对中文场景，额外收集了1,889张图像，生成5,917个问答对，避免了翻译带来的图像-文本不匹配问题，确保文化和语言的适配性。


### **3. 数据收集与标注**


#### **3.1 数据来源**


MME-RealWorld的图像来源于公开数据集和网络，覆盖以下五个领域：


1. **野外光学字符识别（OCR）**：

- 图像来源：从150,259张图像中筛选出3,293张，涵盖街景、商店、广告、书籍、地图、比赛结果等复杂场景。
- 任务：包括5个感知任务（如联系信息、身份信息、产品广告、标牌等）和2个推理任务（如场景理解、角色理解），共5,740个问答对。
- 特点：图像包含复杂文本信息，需要高分辨率以识别细节。

2. **遥感（RS）**：

- 图像来源：从70,000张公开遥感图像中筛选1,298张高分辨率图像，单个图像大小最高达139MB。
- 任务：包括3个感知任务（物体计数、颜色识别、空间关系理解），共3,738个问答对。
- 特点：图像细节丰富，需识别小型物体，挑战模型的细节感知能力。

3. **图表与表格（DT）**：

- 图像来源：从网络筛选2,570张复杂图表图像，如财务报表，包含大量数值和数学信息。
- 任务：包括4个任务（图表感知、表格感知、图表推理、表格推理），共5,933个问答对。
- 特点：任务涉及定位特定值、计算最大/最小值、趋势预测等，考验模型的数学和推理能力。

4. **自动驾驶（AD）**：

- 图像来源：从40,000张车载摄像头图像中筛选2,715张，覆盖多种天气、地理位置和交通场景。
- 任务：包括感知任务（物体识别、属性识别、物体计数，共3,660个问答对）和推理任务（意图预测、交互关系理解、驾驶员注意力理解，共1,334个问答对）。
- 特点：涉及远距离感知和动态交通元素交互，如预测行人或车辆的意图。

5. **监控（MO）**：

- 图像来源：从10,000张公开监控图像中筛选1,601张，涵盖街道、商场、高速公路等场景，涉及多种摄像头和环境（如昼夜、红外图像）。
- 任务：包括3个感知任务（物体计数、物体定位、属性识别，共2,196个问答对）和3个推理任务（对象总数计算、意图推理、属性推理，共498个问答对）。
- 特点：图像具有尺度变化、视角多样性等挑战，测试模型在复杂环境下的鲁棒性。

#### **3.2 标注过程**


- **标注团队**：由25名专业标注者和7名MLLM专家组成，确保标注质量。
- **标注规则**：

所有问题必须基于图像可回答（除特殊设计的“E”选项外），确保问题与图像内容一致。
被提问对象的图像区域不得超过图像总面积的1/10，避免过于显眼，增加难度。
每个标注由至少两名专业研究者交叉检查，防止人为偏见。

- **问答设计**：每个问题提供5个选项（A、B、C、D、E），其中一个为正确答案，三个为干扰项（基于图像文本或类似内容），E选项允许模型拒绝回答，增加任务复杂性。

#### **3.3 中文版本（MME-RealWorld-CN）**


- **问题**：传统中文基准通过翻译英文问题生成，存在图像-文本不匹配和翻译不准确的问题。
- **解决方案**：

从监控、自动驾驶、遥感图像中筛选不含英文信息的图像。
由4名精通中英文的研究者翻译问题和答案。
额外收集939张OCR图像和601张图表/表格图像，内容为中文，生成5,917个问答对。

- **特点**：保留与MME-RealWorld相同的任务类型和难度，专注于中文场景。


### **4. 实验与结果**


#### **4.1 实验设置**


- **模型**：评估了28个MLLM，包括24个开源模型（如InternVL-2、LLaVA、Qwen-VL等）和4个闭源模型（GPT-4o、Gemini 1.5 Pro、Claude 3.5 Sonnet、GPT-4o-mini）。
- **评估指标**：

**平均准确率（Avg）**：按子任务加权平均准确率。
**类均准确率（Avg-C）**：按子任务非加权平均准确率。

- **提示设置**：使用统一的提示模板（见表1），要求模型仅返回正确选项的字母（A、B、C、D、E）。

#### **4.2 主要结果**


- **总体性能**：

没有模型在MME-RealWorld上的准确率超过60%，表明任务极具挑战性。
InternVL-2表现最佳，平均准确率约为50%，优于其他开源和闭源模型。
闭源模型（如GPT-4o）在OCR任务上表现较好（77%准确率），但在其他任务（如遥感、监控）中性能显著下降。

- **领域-specific结果**：

**OCR**（表7）：

GPT-4o在产品广告、书籍/地图/海报等任务中表现最佳（79.66%平均准确率）。
推理任务（如场景理解、角色理解）难度较高，模型准确率普遍低于60%。


**图表与表格（DT）**（表8）：

Claude 3.5 Sonnet在感知任务中表现最佳（67.44%平均准确率），但推理任务（如趋势预测）准确率仅61.20%。
InternVL-2在感知任务中次优（62.80%），但推理任务表现较差（39.00%）。


**自动驾驶（AD）**（表10）：

InternVL-2在感知任务中领先（约50%准确率），但推理任务（如意图预测）难度较大，准确率较低。
闭源模型在复杂交通场景中的表现不如预期，可能受限于图像分辨率处理能力。


**监控（MO）**（表12）：

InternVL-2在物体计数、定位和属性识别任务中表现较好，但推理任务（如意图推理）准确率较低。
任务涉及多种视角和环境（如红外图像），对模型鲁棒性要求高。

- **分析**：

**高分辨率挑战**：闭源模型在高分辨率图像处理上受限（如Claude 3.5 Sonnet的最大分辨率限制），导致在遥感、监控等任务中性能下降。
**推理任务难度**：推理任务（如意图预测、交互关系理解）需要结合视觉和上下文信息，模型普遍表现不佳。
**中文版本**：MME-RealWorld-CN的结果显示，中文场景任务同样具有高难度，模型性能与英文版本一致，未见显著提升。


### **5. 主要贡献与意义**


1. **大规模高分辨率基准**：

- MME-RealWorld是目前最大规模的人工标注多模态基准，包含13,366张高分辨率图像和29,429个问答对，平均分辨率约为2000×1500像素，远超现有基准。
- 高分辨率图像能够捕捉现实世界中的细节，如记分牌文字或遥感图像中的小型物体，适合评估MLLM的细粒度感知能力。

2. **高质量标注**：

- 所有问答对由专业团队手动完成，并经过多轮交叉检查，避免了自动生成标注的噪声问题。
- 严格的标注规则（如对象区域限制、问题基于图像可回答）确保了数据质量和任务难度。

3. **现实世界应用性**：

- 涵盖5个现实场景（OCR、遥感、图表、自动驾驶、监控），任务设计贴近实际需求，如公共安全、财务分析、自动驾驶决策等。
- 任务难度高，即使人类也难以轻松完成，突显了MLLM在现实应用中的不足。

4. **中文基准**：

- MME-RealWorld-CN针对中文场景优化，避免了翻译问题，提供了高质量的中文多模态评估数据。

5. **揭示MLLM局限性**：

- 实验表明，即使最先进的模型（如InternVL-2、GPT-4o）在高分辨率和复杂场景中的性能仍不足60%，凸显了当前MLLM在细节感知和复杂推理方面的短板。
- 为未来MLLM设计提供了方向，如改进高分辨率图像处理能力和推理能力。


### **6. 局限性与未来工作**


- **局限性**：

尽管MME-RealWorld规模较大，但某些子任务的样本量（如推理任务）相对较小，可能影响评估稳定性。
数据收集和标注成本高，限制了进一步扩展的可能性。
部分任务（如遥感图像中的小型物体识别）对模型的计算资源要求较高，可能不适用于所有模型。

- **未来工作**：

扩展更多现实场景，如医疗影像、工业检测等。
引入动态视频数据，测试MLLM在时序多模态任务中的表现。
优化高分辨率图像处理算法，降低计算成本，提升模型适用性。


### **7. 总结**


MME-RealWorld通过提供大规模、高分辨率、人工标注的基准，填补了现有MLLM评估在数据规模、质量和难度上的空白。其聚焦于现实世界的复杂场景，揭示了当前MLLM在高分辨率图像感知和复杂推理任务中的不足。实验结果显示，即使最先进的模型也难以应对这些挑战，准确率未超过60%，为MLLM的未来发展指明了方向。该基准及其中文版本MME-RealWorld-CN为多模态研究提供了宝贵的资源，数据和评估代码已在项目页面（[https://mme-realworld.github.io/）公开。

# **《Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark》**

### **1. 引言与背景**


文章开篇讨论了人工智能通用智能（AGI）的概念，引用Morris等人提出的AGI分级分类，强调**专家级AGI（Expert AGI）**是一个关键里程碑，定义为在广泛任务中达到“至少90%熟练成人的水平”。这种能力可能导致许多行业的劳动力替代，带来显著的经济和社会影响。因此，设计能够评估专家级AGI的基准至关重要。


现有基准如MMLU和AGIEval主要聚焦于纯文本问题，忽略了多模态能力，而现实中的专家任务通常涉及文本和图像的综合处理。大型多模态模型（LMMs）在现有视觉问答（VQA）等基准上表现优异，但在需要专业领域知识和复杂推理的任务上仍显不足。例如，ScienceQA虽然涵盖多学科，但问题多为小学到中学水平，缺乏专家级深度。


为此，文章提出了MMMU基准，专注于大学水平的、多学科的、多模态理解和推理任务，旨在推动下一代多模态基础模型向专家级AGI迈进。



### **2. MMMU数据集概述**


MMMU是一个综合性的多模态基准数据集，具有以下关键特点：


#### **2.1 数据规模与覆盖范围**


- **问题数量**：共11,500道问题，分为开发集（150道）、验证集（900道）和测试集（10,500道）。
- **学科覆盖**：涵盖6大核心学科（艺术与设计、商业、科学、医学与健康、人文与社会科学、技术与工程），30个具体科目，183个子领域。
- **图像类型**：包含30种高度异构的图像类型，如图表、地图、表格、乐谱、化学结构、医学影像等。
- **问题类型**：94.03%为多选题，5.97%为开放式问题，17.62%的问题附带解释。
- **问题难度**：分为简单（28%）、中等（45%）和困难（27%），确保评估模型的深度推理能力。

#### **2.2 四大挑战**


MMMU设计了以下四个核心挑战，以测试多模态模型的极限：


1. **全面性**：覆盖广泛的学科和科目，测试模型的广度。
2. **异构图像类型**：要求模型处理多样化的视觉输入，从照片到专业领域的图表。
3. **交错的文本与图像输入**：问题通常需要联合理解文本和图像，测试多模态整合能力。
4. **专家级感知与推理**：许多问题需要深厚的学科知识和复杂推理，如应用傅里叶变换或平衡理论。

#### **2.3 数据收集与质量控制**


- **收集过程**：

由50名来自不同学科的大学生（包括作者）从在线资源、教科书和讲义中手动收集问题。
选择标准：优先选择视觉输入在学科中具有重要作用的科目，排除如法律、语言学等视觉内容较少的领域。
为避免数据污染，问题答案通常来自单独文档或教科书末尾，确保答案不易被模型直接获取。

- **质量控制**：

**去重**：通过词语重叠和URL相似性检查，剔除重复问题。
**格式校验**：由作者团队进行格式和拼写检查，确保标准化。
**难度筛选**：排除约10%的过于简单问题，确保数据集的挑战性。

#### **2.4 与现有基准的对比**


MMMU与其他多模态基准（如VQA、ScienceQA、MathVista等）相比，具有以下独特优势：


- **广度**：涵盖30种图像类型和30个学科，而非仅限于日常知识或单一领域（如MathVista仅限数学）。
- **深度**：要求大学水平的专业知识和复杂推理，而非常识或简单推理。
- **多模态性**：强调文本与图像的交错理解，测试模型的综合能力。


### **3. 实验设计与结果**


文章对28个开源LMMs以及闭源模型（如GPT-4V、Gemini）进行了评估，采用零样本（zero-shot）设置以测试模型的泛化能力。以下是主要实验结果和发现：


#### **3.1 评估模型**


- **开源模型**：包括Kosmos2、LLaVA-1.5、BLIP-2、InstructBLIP、CogVLM等。
- **闭源模型**：GPT-4V、Claude 3 Opus、Gemini 1.5 Pro、GPT-4o等。
- **文本模型基线**：测试了如GPT-4、Llama2-7B等文本LLM，并尝试通过OCR或图像描述增强其性能。
- **人类专家**：邀请90名大学高年级学生（每科目3名专家）完成900道验证集问题，允许查阅教科书但禁止联网搜索。

#### **3.2 主要结果**


- **挑战性**：MMMU对当前模型构成重大挑战。最佳人类专家在验证集上达到88.6%的准确率，远超所有模型。GPT-4V仅达到55.7%，Gemini Ultra为59%，表明模型与人类专家之间存在显著差距。
- **开源与闭源模型差异**：开源模型（如BLIP-2、LLaVA-1.5）准确率约为34%，远低于GPT-4V。但开源模型（如LLaVA-1.6-34B、InternVL-Chat-V1.2）在测试集上分别达到44.7%和46.2%，缩小了与闭源模型的差距。
- **OCR/描述增强效果**：为文本LLM添加OCR或图像描述未显著提升性能，表明MMMU需要深度的文本-图像联合理解。
- **学科表现差异**：

在艺术与设计、人文与社会科学等学科（图像较简单，推理需求较低），模型表现较好。
在科学、医学与健康、技术与工程等学科（图像复杂，推理要求高），模型表现较差。

- **图像类型表现**：GPT-4V在所有图像类型上均领先，但在几何形状、乐谱、化学结构等不常见类型上表现接近随机猜测。开源模型在照片、绘画等常见类型上表现较好。
- **难度级别表现**：在“简单”问题上，GPT-4V准确率达76.1%，领先开源模型。但在“困难”问题上，所有模型表现差距缩小，表明复杂任务对先进模型的挑战依然巨大。


### **4. 错误分析**


文章对GPT-4V的150个错误案例进行了详细分析，由专家标注错误原因。错误分布如下：


- **感知错误（35%）**：

**基础感知错误**：如误判“从左到右、从上到下”的顺序（见附录图6）。
**领域特定感知错误**：如在计算机科学中无法正确识别确定性有限自动机的“接受状态”（附录图83）。

- **知识缺失（29%）**：模型缺乏特定领域知识，如在材料力学中错误使用剪切应变公式（附录图91）。
- **推理错误（26%）**：即使正确理解文本和图像，逻辑或数学推理步骤错误，如在工程动力学中错误计算势能（附录图93）。
- **其他错误**：包括文本理解错误（6%）、拒绝回答（3%）、标注错误（2%）和答案提取错误（1%）。

#### **关键洞察**：


- **文本-图像平衡**：模型倾向于优先处理文本，忽略视觉信息，导致错误（如附录图67中误判卡通图像的叙事）。
- **grounding 挑战**：指代视觉输入中的特定元素仍具挑战性。
- **复杂推理困难**：涉及长推理链或大量计算的任务对模型构成重大挑战。


### **5. 示例案例（附录）**


文章附录提供了具体错误和正确案例，展示模型在不同任务中的表现：


- **材料力学（图91）**：

**问题**：计算矩形板变形为平行四边形后角A和角B的平均剪切应变。
**GPT-4V错误**：错误使用简化公式（γ_xy = δy/δx），忽略基于角度变化的正确公式，导致答案错误（预测：0.0125 rad；正确答案：0.0292 rad）。
**错误原因**：缺乏领域知识。

- **控制系统（图92）**：

**问题**：计算统一反馈系统的稳态误差。
**GPT-4V正确**：正确应用终值定理，得出稳态误差为0（正确答案：A）。

- **工程动力学（图93）**：

**问题**：计算两球沿光滑圆形轨道滑至水平位置的速度和法向力。
**GPT-4V错误**：错误计算势能（3mgR而非mgR），导致速度和力计算错误（预测：√2gR, 2mg；正确答案：√gR, 2mg）。
**错误原因**：推理错误。

- **工程动力学（图94）**：

**问题**：计算男孩投掷石头以刚好越过障碍物的最小水平速度。
**GPT-4V错误**：推理步骤错误，关注无关的时间计算（落26米），导致速度错误（预测：24.014 m/s；正确答案：28.014 m/s）。
**错误原因**：推理错误。


### **6. 结论与局限性**


- **意义**：MMMU通过评估感知、知识和推理能力，为专家级AGI提供了一个综合性基准，填补了现有基准在深度和多模态性方面的不足。
- **局限性**：

手动收集可能引入偏见。
大学水平任务可能不足以完全涵盖专家级AGI的评估需求。
未直接映射到“90%熟练成人”的性能标准。

- **未来工作**：

改进视觉感知能力，尤其是在不常见图像类型上。
增强领域特定知识的表示。
提升复杂推理能力，特别是长推理链和数学计算。
优化文本与图像的联合理解，减少对文本的过度依赖。


### **7. 附录内容**


- **子领域列表**：详细列出30个科目的183个子领域，如生物化学、财务会计、计算机网络等（表12）。
- **图像类型分布**：图表（3184个样本）和表格（2267个）占主导，DNA序列（20个）等较少（图95）。
- **少样本学习结果**：OpenFlamingo和Otter在少样本学习中表现下降，表明MMMU问题复杂，模型难以捕捉模式（表14）。
- **数据标注协议**：规定了数据来源（教科书、在线资源）、问题类型（多选、开放）、图像要求（多样化）、质量控制（去重、格式校验）和版权合规性（附录H）。


### **关键结论**


- **MMMU的独特性**：通过结合大学水平知识、30种图像类型和复杂推理，MMMU填补了常识基准与专家任务之间的空白。
- **性能差距**：即使是先进模型如GPT-4V也远低于人类专家，表明LMMs在专家级任务上有很大提升空间。
- **研究方向**：需要改进感知能力、领域知识和推理能力，以推动多模态模型向专家级AGI发展。

# 《GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering》

### 1. 背景与研究动机


**视觉问答（VQA）的挑战与现有数据集的局限性**
视觉问答（VQA）是一项结合视觉和语言理解的任务，要求系统通过对图像的分析回答自由形式的自然语言问题。这需要系统具备物体识别、常识理解、关系推理等多种能力。然而，现有VQA数据集（如VQA 1.0、VQA 2.0、CLEVR等）存在以下关键问题：


- **数据偏差**：数据集中的答案分布存在显著的统计偏差（例如，大多数西红柿是红色的，桌子是木质的），导致模型倾向于利用这些偏差进行“猜测”，而不是真正理解图像内容。例如，盲模型（仅基于问题而非图像）在VQA 1.0上可达到50%的准确率，而在VQA 2.0上，开放性问题的正确率也能达到27%。
- **问题简单性**：许多问题的语言和语义复杂度较低，通常仅需物体识别即可回答，缺乏对多步骤推理或复杂语义理解的需求。
- **缺乏结构化信息**：现有数据集通常不提供问题的结构化表示或详细的图像标注，难以深入分析模型的错误原因或行为模式。
- **评估单一**：传统VQA任务主要依赖准确率（accuracy）作为评估指标，忽视了模型在一致性、合理性等方面的表现，无法全面反映模型的视觉理解能力。

为解决这些问题，作者提出了GQA数据集，旨在通过更复杂的推理问题、平衡的答案分布和新的评估指标，提供一个更具挑战性、更能反映真实视觉推理能力的基准。



### 2. GQA数据集的主要贡献


GQA（Grounded Question Answering）数据集的目标是推动视觉推理和组合式问答的研究，其主要贡献包括以下三个方面：


1. **GQA数据集**

- GQA包含113,018张真实图像（来源于COCO和Flickr），以及2266.9万个多样化的问题，涵盖多种推理技能，如物体识别、属性判断、空间推理、逻辑推理和比较推理。
- 每个问题都附有功能程序（functional program），明确表示其语义结构，便于控制问题类型和答案分布。
- 图像配有Visual Genome场景图（scene graph）标注，详细描述物体、属性和关系，提供丰富的语义信息。
- 答案附有文本和视觉解释，支持模型的透明性和可解释性。

2. **问题生成方法**

- 开发了一种强大的问题生成引擎，利用Visual Genome的场景图和精心设计的语言模式，生成多样化且具有组合性的问题。
- 通过规范化场景图和使用524种结构模式（涵盖117个问题组），确保问题在语义和语言上的多样性。
- 引入了“候选引用”（candidate references）和“干扰项”（decoys），增强问题的复杂性和挑战性，避免简单猜测。

3. **新的评估指标**

- 提出了五种新的评估指标（一致性、有效性、合理性、分布匹配、视觉 grounding），以补充传统准确率指标，全面评估模型的推理能力和行为。
- 这些指标能够揭示模型在语义理解、一致性推理和视觉注意力分配等方面的不足。


### 3. GQA数据集的构建过程


GQA数据集的构建过程分为四个主要步骤，结合场景图规范化、问题生成、答案平衡和功能表示，确保数据集的高质量和挑战性。


#### 3.1 场景图规范化（Scene Graph Normalization）


- **输入**：使用Visual Genome的场景图标注，覆盖113,000张图像（包括COCO和Flickr的图像，以及通过众包额外收集的5,000个隐藏场景图用于测试集）。
- **处理**：

**词汇清理**：移除停用词、修正拼写错误、合并同义词，过滤稀有或模糊概念，建立包含1740个物体、620个属性和330个关系的清晰语义本体。
**分类与增强**：将词汇分类为物体、属性和关系，并添加语义和语言信息（如词性、复数形式、同义词），以支持生成语法正确的问题。
**边缘修剪**：通过物体检测置信度、n-gram频率、共现统计、词嵌入距离和人工筛选，移除不自然或不准确的关系（如“woman, in, shirt”）。
**信息增强**：添加绝对和相对位置信息（如物体在图像边缘的标注、物体间的相对位置），以及全局信息（如场景的地点或天气）。

- **输出**：干净、统一且语义丰富的场景图，为问题生成提供可靠的基础。

#### 3.2 问题生成引擎（Question Engine）


- **核心资源**：

**场景图**：提供物体、属性和关系的语义内容。
**结构模式**：包含524种模式（117个问题组），每组包括功能程序、自然语言表述和短/长答案模板。

- **生成过程**：

从250个手动构建的模式开始，扩展到274个从VQA 1.0提取的自然模式，通过模板化生成多样化问题。
使用概率性语言规则（如可选短语、替换表达）增加语言多样性。
为每个物体生成直接引用（direct references，如“the bear”）和间接引用（indirect references，如“the white bear on the left”），支持多步骤推理问题。
引入干扰项（如错误属性或不存在的物体），生成否定性或逻辑推理问题（如“Is the apple green?”当苹果实际为红色时）。

- **结果**：生成2266.9万个多样化、语法正确且具有挑战性的问题，覆盖图像的各个方面。

#### 3.3 功能表示与蕴含关系（Functional Representation and Entailment）


- **功能程序**：每个问题都关联一个功能程序，描述回答问题所需的推理步骤。例如，“What color is the apple on the white table?”的功能程序为：select: table → filter: white → relate(subject, on): apple → query: color。
- **优势**：

便于分析问题类型和推理复杂度。
支持蕴含关系（entailment）的计算，如知道苹果是红色，可以推断它不是绿色。
帮助平衡答案分布，减少偏差。

- **蕴含关系**：通过功能程序定义问题间的逻辑关系，用于评估模型的一致性。例如，若模型回答“苹果是红色”，则不应在后续问题中回答“苹果是绿色”。

#### 3.4 采样与平衡（Sampling and Balancing）


- **目标**：减少答案分布的偏差，使“猜测”策略失效。
- **方法**：

将问题按全局标签（如“颜色问题”）和局部标签（如“苹果-颜色”）分组。
对每个组的答案分布进行平滑处理，使用拒绝采样（rejection sampling）调整分布，使其更均匀，同时保留真实世界的倾向性。
通过迭代调整高频答案的权重，增加低频答案的比重（熵增加72%）。

- **结果**：从2266.9万问题中采样出170万个平衡问题，分为训练（70%）、验证（10%）、测试（10%）和挑战（10%）集。


### 4. GQA数据集的特点


- **真实图像与丰富语义**：基于真实图像（COCO和Flickr），结合场景图提供详细的语义标注，兼顾真实性和语义丰富性。
- **多样化推理技能**：问题涵盖物体存在、属性判断、类别识别、关系推理和全局场景理解等多种技能。
- **组合性问题**：52%的问题需要多步骤推理（如空间、逻辑、比较），相比VQA 2.0的3%显著提高。
- **平衡的答案分布**：通过平滑技术减少偏差，降低模型对统计规律的依赖。
- **详细标注**：每个问题附有功能程序、文本/视觉解释和图像区域指针，支持可解释性和模型诊断。
- **问题类型**：

**结构类型**：验证（verify）、查询（query）、选择（choose）、逻辑（logical）、比较（compare）。
**语义类型**：物体（object）、属性（attribute）、类别（category）、关系（relation）、全局（global）。


### 5. 新的评估指标


GQA引入了五种新的评估指标，以更全面地评估模型的视觉推理能力：


1. **一致性（Consistency）**

- **定义**：检查模型在相关问题上的回答是否一致。例如，若模型回答“苹果是红色”，则在“Is the apple green?”中应回答“否”。
- **方法**：利用功能程序计算蕴含关系，评估模型在正确回答的问题的蕴含问题上的准确率。
- **结果**：人类一致性达98.4%，而最佳模型仅为81.59%，显示模型在语义理解上的不足。

2. **有效性（Validity）**

- **定义**：检查答案是否在问题的语义范围内（如颜色问题应回答颜色）。
- **结果**：模型有效性为96.02%-96.39%，人类为98.9%，表明模型偶尔回答无效答案。

3. **合理性（Plausibility）**

- **定义**：检查答案是否合理（如苹果的颜色不应为紫色）。
- **方法**：基于数据集中物体与属性的共现统计，判断答案的合理性。
- **结果**：模型合理性为84.25%-87.30%，人类为97.2%，显示模型缺乏对罕见答案的理解。

4. **分布匹配（Distribution）**

- **定义**：使用卡方统计量测量模型预测的答案分布与真实分布的匹配程度。
- **结果**：领先模型在分布匹配上优于基线，表明其能捕捉更细微的分布趋势。

5. **视觉Grounding（Grounding）**

- **定义**：评估注意力模型是否关注与问题相关的图像区域。
- **方法**：测量模型在问题或答案指向的图像区域上的注意力概率。
- **结果**：基于物体的特征（如BottomUp和MAC）获得约80%的grounding得分，而基于空间特征的模型仅为43%，显示物体特征的优越性。


### 6. 实验与分析


#### 6.1 基线与模型评估


- **测试模型**：

**基线**：盲LSTM（仅问题）、聾CNN（仅图像）、LSTM+CNN、基于全局/局部先验的模型。
**先进模型**：BottomUp（2017 VQA挑战赛冠军）、MAC（适用于CLEVR的组合注意力模型）。
**人类**：通过Amazon Mechanical Turk收集4000个问题的人类回答。

- **结果**（见表1）：

盲LSTM准确率为41.07%，显示其依赖问题偏差。
BottomUp和MAC分别为49.74%和54.06%，远低于人类（89.3%）。
按问题类型分析，模型在开放性查询问题（22.69%-38.91%）和关系问题（33.24%-46.16%）上表现较差。
人类在所有类型上表现优异，尤其在一致性（98.4%）和合理性（97.2%）上远超模型。

#### 6.2 迁移性能


- **GQA到VQA**：在GQA上训练的MAC模型在VQA上的准确率为52.1%（未微调）和60.5%（微调），接近VQA训练的模型（68.3%），显示GQA问题的多样性和真实性。
- **VQA到GQA**：在VQA上训练的MAC模型在GQA上的准确率为39.8%（未微调）和46.5%（微调），表明GQA更具挑战性。

#### 6.3 进一步诊断


- **问题长度**：文本长度（单词数）与准确率正相关，可能因长问题提供更多线索；语义长度（推理步骤数）与准确率负相关，显示组合性问题的难度。
- **训练集规模**：增加训练数据显著提升性能，表明GQA仍未达到性能饱和。
- **输入表示**：使用更高级的语义表示（如场景图嵌入）比空间特征（CNN）或物体特征（Faster R-CNN）获得更高准确率，显示语义信息的重要性。
- **MAC网络长度**：更长的MAC网络（更多单元）在GQA任务上表现更好，验证了其组合性推理能力。


### 7. GQA与VQA 2.0的比较


- **问题长度**：GQA平均7.9个单词，VQA 2.0为6.2个，GQA问题更长且复杂。
- **语言复杂性**：GQA包含更多动词（1.6 vs 1.4）、名词（2.5 vs 1.9）、形容词（0.7 vs 0.6）和介词（1.1 vs 0.5），显示更高组合性。
- **推理需求**：GQA包含51.6%关系问题、22.4%空间问题和52%组合性问题，远高于VQA 2.0的19.5%、8%和3%。
- **问题类型**：GQA专注于客观、基于图像的推理问题，排除主观或外部知识问题（如“为什么”问题），更适合控制实验。


### 8. 结论与意义


GQA数据集通过真实图像、组合性问题、平衡的答案分布和新的评估指标，为视觉推理和问答研究提供了新的基准。其主要优势包括：


- **挑战性**：减少偏差，强调多步骤推理，降低“猜测”策略的有效性。
- **可解释性**：功能程序和视觉/文本解释支持模型诊断和透明性。
- **全面评估**：新指标揭示模型在一致性、合理性和注意力分配上的不足。
- **推动研究**：为开发更具组合性、解释性和一致性的模型提供了资源。

作者希望GQA能激励视觉知识提取与问答的深度整合，推动场景理解和VQA领域的发展。



### 9. 局限性与未来方向


- **局限性**：

GQA问题虽多样，但仍为生成问题，可能缺乏人类提问的自然性。
不涵盖主观问题或需要外部知识的问题（如意图或OCR相关问题）。
场景图的完整性依赖Visual Genome，可能存在遗漏。

- **未来方向**：

结合自然语言生成技术，生成更自然的组合性问题。
扩展到更多问题类型，如主观或外部知识问题。
开发利用功能程序和场景图的模型，提升推理能力和可解释性。


### 10. 补充说明


- **数据集获取**：GQA数据集及相关信息可在[visualreasoning.net](http://visualreasoning.net/)获取。
- **致谢**：研究得到Facebook、Samsung和DARPA的支持，作者感谢多位研究者的反馈和建议。

# 《A Diagram Is Worth A Dozen Images》

由Aniruddha Kembhavi等人于2016年发表，研究了图表理解和推理的问题，提出了新的表示方法Diagram Parse Graphs (DPG)以及两种基于神经网络的模型，用于解决图表句法解析和语义推理的任务。

### 1. 背景与研究意义


**图表的独特性**
文章指出，图表（diagrams）作为一种表达复杂概念、关系和事件的工具，在科学教育、工程设计等领域被广泛使用。与自然图像（natural images）相比，图表具有以下特点：


- **信息表达的针对性**：图表设计目的是清晰传递信息，通常去除背景杂乱、复杂纹理等无关信息，突出关键元素（如箭头、文本框、图示等）。
- **复杂现象的表示**：图表能表达自然图像难以呈现的复杂现象，如时间变化、阶段转换或对象间的高阶关系（例如食物链中的捕食关系或水循环中的相态变化）。
- **多样性与挑战性**：图表种类繁多（如科学教科书中的水循环图、食物网图等），即使同一类别（如水循环图）内部也存在显著的视觉差异（见图1）。

**研究空白**
尽管自然图像理解在计算机视觉领域已取得显著进展，图表理解却鲜有研究。传统方法（如20世纪80-90年代的研究）多依赖手工规则或特定图表类型，缺乏通用性。近年来，虽然抽象图像的研究有所进展，但它们仍主要描绘现实场景，与图表描绘的复杂现象有本质区别。


**研究目标**
本文定义了图表理解的两个核心任务：


1. **句法解析（Syntactic Parsing）**：识别图表的组成部分（如文本、箭头、图示）及其句法关系，生成结构化表示。
2. **语义推理（Semantic Interpretation）**：将图表的组成部分和关系映射到现实世界的概念，并通过问答任务验证推理能力。

为此，文章提出了Diagram Parse Graphs (DPG)作为图表结构化表示，并开发了两个神经网络模型：


- **DSDP-NET**：用于句法解析，生成DPG。
- **DQA-NET**：用于语义推理，回答图表相关问题。


### 2. Diagram Parse Graphs (DPG)


**DPG的定义**
DPG是一种图结构表示，用于编码图表的组成部分及其关系，灵感来源于Engelhardt [7]提出的图形表示框架。DPG的节点和边定义如下：


- **节点**：表示图表的四种基本组成部分：

**Blobs**：图示元素（如动物、植物的插图）。
**Text Boxes**：文本框（如标签、标题）。
**Arrows**：箭头（表示关系或流向）。
**Arrow Heads**：箭头头部（与箭尾分离以统一表示单头、多头或无头箭头）。

- **边**：表示组成部分之间的关系，共定义了10种关系类型（见表1），包括：

**Intra-Object Label (R1)**：命名整个对象的文本框。
**Inter-Object Linkage (R4)**：通过箭头连接的两个对象。
**Arrow Descriptor (R6)**：描述箭头所指过程的文本框。
**Image Title (R7)**：整个图表的标题等。

**DPG的作用**
DPG通过节点和边的组合，捕获图表的句法结构。例如，在食物网图中，DPG可以表示“狐狸吃兔子”的关系（Inter-Object Linkage）。在水循环图中，DPG可以表示“蒸发”过程（Arrow Descriptor）。图6展示了DPG的示例。

### 3. 句法解析：DSDP-NET


**任务定义**
句法解析的目标是将图表映射到DPG，识别其组成部分及其句法关系。由于图表种类繁多且内部差异大，这一任务具有挑战性。


**DSDP-NET模型**
文章提出了Deep Sequential Diagram Parser Network (DSDP-NET)，基于长短期记忆网络（LSTM）实现DPG的生成：


- **模型结构**（见图2）：

包含两层堆叠LSTM，每层512维隐藏状态。
LSTM前后分别连接全连接层（带ReLu激活函数）和softmax层。
输入为候选关系的特征向量（92维），包括组成元素的坐标、检测得分、重叠率等。

- **工作原理**：

通过对象检测器和关系分类器生成大量节点和边候选。
DSDP-NET以序列方式处理候选关系，逐一决定是否将其加入DPG，利用全局上下文（如空间重叠、覆盖率）优化选择。

- **训练**：

从训练图表中采样关系序列（每个图表约400,000样本），利用关系得分作为采样权重。
使用RMSProp优化交叉熵损失，初始学习率为0.0002。

- **测试**：

按候选关系的得分排序，依次输入DSDP-NET，构建最终DPG。

**基线方法**


- **Greedy Search**：基于提案得分贪婪添加节点和边，使用退出模型判断是否停止。
- *A Search**：基于随机森林模型的评分和到目标DPG的距离，优化节点和边的添加。
- **Direct Regression**：尝试直接回归DPG（类似YOLO），但效果不佳。

**评估 Cyril**
采用Jaccard Index for Graphs (JIG) 评分，DSDP-NET的平均JIG得分为51.45，显著优于Greedy Search（28.96）和A* Search（41.02）（见表2）。

### 4. 语义推理：DQA-NET


**任务定义**
语义推理通过图表问答任务（Diagram Question Answering, DQA）评估，将DPG的句法关系映射到语义概念并进行推理。例如，回答食物网图中“植物减少对狐狸种群的影响”需要理解“捕食”关系并推理其后果。


**DQA-NET模型**
DQA-NET是一个基于注意力的神经网络，用于回答图表问题（见图3）：


- **模型结构**：

**问题嵌入模块**：将问题和选项组合成语句，通过LSTM生成50维嵌入向量。
**图表嵌入模块**：从DPG提取关系，通过LSTM生成嵌入向量。
**注意力模块**：计算问题语句与DPG关系之间的相似度，输出答案概率。

- **训练**：

使用GloVe词向量（300维），LSTM单层50维隐藏单元。
采用随机梯度下降优化交叉熵损失，学习率从0.01开始，每25轮减半，共100轮。

**基线方法**


- **VQA（LSTM Q+I）**：基于自然图像的视觉问答模型，使用VGG-19提取图像特征，与问题嵌入进行匹配。

**结果**
DQA-NET在AI2D数据集上的准确率为38.47%，优于VQA（VQA数据集训练：29.06%，AI2D数据集训练：32.90%）（见表2）。这表明DPG有效编码了图表的高级语义。

### 5. AI2D数据集


**数据集概述**
文章创建了AI2 Diagrams (AI2D)数据集，包含：


- **5,000+张科学教科书图表**：涵盖1-6年级科学主题。
- **150,000+个标注**：包括组成元素（&gt;118,000）和关系（&gt;53,000）。
- **15,000+个多选题**：与图表相关，用于评估语义推理。
- **划分**：4,000张训练集，1,000张测试集。

**数据采集与标注**


- 通过Google Image Search收集图造型，以教科书章节标题为种子词。
- 使用Amazon Mechanical Turk分阶段标注，分为六步（如节点、关系、问题等），确保标注质量。

**图4示例**
展示了数据集中的一个图表及其丰富的标注和相关问题。

### 6. 实验与结果


**组成元素检测**


- **Canvas**：使用RGB、纹理和熵特征的随机森林分类器，AP为0.9142。
- **Blobs**：结合多尺度组合分组（MCG）和canvas概率图，AP为0.7829，优于Edge Boxes（0.02）。
- **Arrow Tails**：使用边界检测、Hough变换和CNN，AP为0.6748。
- **Arrow Heads**：滑动窗口加CNN，优于随机森林基线。
- **Text**：结合OCR和单字符CNN，显著提高精度（0.89）和召回率（0.75）。

**句法解析**
DSDP-NET的JIG得分（51.45）优于基线，表明其序列化LSTM方法在处理复杂DPG结构方面的优势。


**图表问答**
DQA-NET在AI2D数据集上的表现优于VQA基线，显示了DPG在语义推理中的重要性。



### 7. 贡献与意义


**主要贡献**


1. 定义了图表理解的两个新任务：句法解析和语义推理。
2. 提出DPG表示，用于编码图表的句法结构。
3. 开发DSDP-NET，利用LSTM实现DPG的序列生成。
4. 开发DQA-NET，利用注意力机制回答图表问题。
5. 创建AI2D数据集，包含丰富标注和问题，促进未来研究。

**意义**


- 图表理解引入了超越自然图像理解的新挑战，如复杂关系和高阶推理。
- DPG提供了一种统一的表示方法，适用于多样化的图表。
- AI2D数据集为图表理解研究提供了宝贵的资源。

**未来工作**


- 结合图表知识和常识增强DQA模型。
- 探索更复杂的图表类型和推理任务。


### 8. 总结

这篇文章提出了一种全新的图表理解框架，通过DPG、DSDP-NET和DQA-NET解决句法解析和语义推理问题。AI2D数据集为该领域提供了重要的研究资源。实验结果表明，提出的模型在性能上显著优于基线方法，展示了DPG表示和神经网络模型在图表理解中的潜力。这一研究为计算机视觉和自然语言处理领域开辟了新的方向，尤其适用于教育、科学和工程领域的复杂图表分析。

# 《Evaluating Object Hallucination in Large Vision-Language Models》


### 一、研究背景与问题提出


大型语言模型（LLMs）在自然语言处理任务中表现卓越，激发了研究者将强大的语言模型与视觉模块结合，开发出大型视觉-语言模型（LVLMs），以提升复杂多模态任务的性能。LVLMs通过结合视觉编码器和语言模型，能够处理图像和文本输入，完成如图像描述（image captioning）和视觉问答（VQA）等任务。然而，研究发现，LVLMs存在对象幻觉问题，即生成的描述中包含与目标图像不一致或不存在的对象。这种现象可能源于其核心组件（LLMs和视觉-语言预训练模型VLPMs）的幻觉倾向，严重影响模型在现实应用中的可靠性和用户体验。例如，自动驾驶系统若因幻觉误判环境，可能导致安全问题。因此，本文旨在系统评估LVLMs的对象幻觉问题，并提出更有效的评估方法。


### 二、研究方法


#### 1. 初步评估：使用CHAIR指标


文章首先采用现有的Caption Hallucination Assessment with Image Relevance（CHAIR）指标，在MSCOCO数据集上评估五种代表性LVLMs（mPLUG-Owl、LLaVA、MultiModal-GPT、MiniGPT-4、InstructBLIP）的对象幻觉程度。CHAIR指标通过计算生成描述中不存在于图像中的对象比例（CHAIR_I）和包含幻觉对象的描述比例（CHAIR_S）来衡量幻觉程度。实验使用两种指令：


- **I1**：生成简短的图像描述。
- **I2**：提供图像的简要描述。

结果（表1）显示：


- 大多数LVLMs存在严重的对象幻觉问题，甚至比小型VLPMs（如OSCAR、VinVL）更严重。例如，LLaVA在I1下的CHAIR_S值为32.4，远高于OSCAR的13.0。
- InstructBLIP表现最佳，幻觉程度较低，可能是因为其使用了多样化的公共数据集进行指令微调，而其他模型依赖LLM生成的较长、可能包含幻觉的合成指令。
- CHAIR指标的不稳定性：不同指令（I1和I2）导致结果差异较大，例如LLaVA在I2下的CHAIR_I值（18.2）远高于I1（10.5）。此外，CHAIR需要复杂的人工解析规则，难以适应LVLMs的生成风格，可能导致误分类。

#### 2. 幻觉原因分析


文章提出两个假设，探讨视觉指令数据对幻觉的影响：


- **假设1**：LVLMs倾向于幻觉视觉指令数据集中频繁出现的对象。
- **假设2**：LVLMs倾向于幻觉与图像中真实对象频繁共现的对象。

通过在MSCOCO数据集上的定性和定量分析验证：


- **定性分析**（图2）：绘制了MSCOCO中前十个频繁出现对象及其幻觉次数，以及与“餐桌”共现对象的幻觉次数。结果显示，对象出现或共现频率越高，幻觉次数越多。例如，mPLUG-Owl对频繁对象的幻觉比例显著。
- **定量分析**（表2）：使用Top-k命中率（HR@k）衡量幻觉对象与频繁出现/共现对象的相关性。结果表明，约50%的幻觉对象属于MSCOCO前10个频繁对象，60%属于与真实对象共现的前10个对象，验证了假设。

#### 3. 新评估方法：POPE


针对CHAIR的不稳定性，文章提出了一种基于轮询的对象探测评估方法（Polling-based Object Probing Evaluation, POPE），将幻觉评估转化为二分类任务，通过询问简单的是/否问题（如“图像中有椅子吗？”）评估LVLMs。


**POPE流程**（图3）：


1. **对象提取**：从图像中提取真实对象（通过人工标注或自动分割工具如SEEM）。
2. **负采样**：为不存在的对象生成问题，采用三种采样策略：

- **随机采样**：随机选择不存在的对象。
- **流行采样**：选择数据集中最常见的k个不存在对象。
- **对抗采样**：选择与真实对象高频共现的k个不存在对象。

3. **问题构建**：为每个图像生成6个问题（真实对象和不存在对象各3个），答案为“是”或“否”。
4. **评估指标**：使用准确率（Accuracy）、精确率（Precision）、召回率（Recall）和F1分数，其中F1为主要指标。

**POPE结果**（表3）：


- InstructBLIP在所有采样策略下表现最佳（F1分数最高达89.29），而mPLUG-Owl、LLaVA和MultiModal-GPT的F1分数低于70，显示严重幻觉问题。
- 模型倾向于回答“是”（Yes比例高达99%），表明过度自信，导致对“否”问题的准确率较低。
- 从随机到流行再到对抗采样，模型性能逐渐下降，证实了LVLMs更容易幻觉频繁出现或共现的对象。

**POPE优势**：


- **稳定性**（表4）：POPE对不同提示模板的F1分数标准差（0.78）远低于CHAIR（3.22）。
- **可扩展性**（表5）：结合SEEM分割工具，POPE可应用于无标注数据集（如A-OKVQA、GQA），结果趋势与标注数据一致，尽管自动分割的细粒度可能使任务更具挑战性。
- **一致性**：POPE的“是/否”回答与模型生成描述高度一致，例如InstructBLIP的1303个“否”回答中，无一出现在描述中。

#### 4. 幻觉对视觉任务的影响


文章进一步探讨幻觉对视觉问答（VQA）和图像描述任务的影响（表6、表12）：


- InstructBLIP在POPE和VQA任务中均表现最佳，表明其指令微调有效。
- MiniGPT-4在POPE上的F1分数高于LLaVA，但在VQA任务中表现较差，可能因其指令数据集仅限于图像描述数据，缺乏复杂视觉问题。
- 图像描述任务的结果与POPE评估一致，表明对象幻觉影响其他视觉任务的表现。

### 三、主要贡献


1. **首次系统性研究**：对多个代表性LVLMs的对象幻觉问题进行了实证评估，揭示其严重性。
2. **幻觉原因分析**：发现视觉指令数据集中对象的频率和共现关系是幻觉的主要影响因素。
3. **提出POPE方法**：提供了一种更稳定、灵活的评估方法，克服了CHAIR的局限性，可扩展到无标注数据集。

### 四、局限性


1. **研究范围**：仅聚焦对象幻觉，未涵盖LVLMs的其他能力，无法全面反映模型性能。
2. **数据规模**：由于计算资源限制，仅在部分验证集上评估，结果可能受数据分布影响。
3. **答案解析**：POPE依赖“是/否”回答，但LVLMs可能不总明确输出这些词，影响评估准确性。
4. **自动分割**：SEEM分割的标签集与人工标注可能不一致，导致评估结果偏差。
5. **模型覆盖**：仅评估了部分LVLMs，未包括最新或闭源模型。

### 五、结论与展望


本文揭示了LVLMs的对象幻觉问题及其与视觉指令数据的关联，提出的POPE方法为更可靠的评估提供了新思路。尽管存在局限性，研究强调了开发更可靠、与人类对齐的LVLMs的重要性。未来可扩展POPE到更细粒度的幻觉评估（如对象属性、数量），并覆盖更多模型和数据集。

# 《SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension》

### 1. 文章背景与研究意义


#### 1.1 研究背景


近年来，大规模语言模型（LLMs）在文本理解、推理和生成方面展现出卓越的能力。基于这些强大的LLMs，多模态大语言模型（MLLMs）通过整合视觉和语言数据，进一步扩展了模型在图像和视频理解与生成任务上的能力。然而，目前对MLLMs的评估主要依赖于定性示例或不完全适合开放式输出的传统基准（如VQAv2）。这些评估方法存在以下问题：


- **定性评估**：依赖少量示例，缺乏系统性和全面性。
- **传统基准局限**：如VQAv2要求模型输出与简短的参考答案完全匹配，不适合评估生成型MLLMs的开放式输出。
- **现有基准规模小**：如MME和MMBench的样本量较小（少于3K），导致评估统计不稳定。
- **主观性与效率问题**：部分基准（如LVLM-eHub和LAMM）依赖人工或GPT评估，增加了主观性和成本，降低了效率。

为了解决这些问题，文章提出了**SEED-Bench**，一个大规模、客观的多模态基准，专注于评估MLLMs的**生成性理解能力**，为未来的多模态模型研究提供方向。


#### 1.2 研究意义


- **全面评估**：SEED-Bench包含19K个人工标注的多选题，覆盖12个评估维度，涵盖图像和视频的**空间理解**和**时间理解**，是现有基准（如MME、MMBench）的6-9倍规模。
- **客观高效**：通过多选题形式和自动化评估策略，避免了人工或GPT干预，降低了主观性，提高了评估效率。
- **推动研究**：通过揭示现有MLLMs的局限性，SEED-Bench为开发更强大的多模态模型提供了洞见，并通过持续维护的排行榜为研究社区提供评估平台。


### 2. SEED-Bench 的核心内容


#### 2.1 基准概述


SEED-Bench 是一个专为评估生成型多模态大语言模型（MLLMs）设计的基准，包含以下关键特点：


- **规模**：19,242个多选题，远超MME（2,914个）和MMBench（2,974个）。
- **模态**：覆盖图像和视频两种模态，评估模型的空间和时间理解能力。
- **评估维度**：共12个维度，分为9个空间理解维度和3个时间理解维度（见下表）。
- **数据质量**：通过自动过滤和人工验证，确保问题的高质量和答案的准确性。
- **评估方式**：采用基于对数似然度的答案排名策略，客观高效，不依赖模型的指令跟随能力。

#### 2.2 评估维度


SEED-Bench 的12个评估维度详细如下（见表2）：


**空间理解（9个维度）**：


1. **场景理解**：评估模型对图像整体信息的理解能力，如图像的整体场景描述。
2. **实例识别**：测试模型是否能识别图像中特定对象的存在或类别。
3. **实例属性**：考察模型对对象属性（如颜色、形状、材质）的理解。
4. **实例位置**：要求模型准确确定指定对象的绝对位置。
5. **实例计数**：测试模型统计图像中特定对象数量的能力。
6. **空间关系**：评估模型识别图像中两个对象相对空间关系的能力。
7. **实例交互**：考察模型对图像中对象或人物之间状态或交互关系的理解。
8. **视觉推理**：测试模型基于视觉信息进行推理的能力，需结合常识知识。
9. **文本识别**：评估模型识别图像中文字内容的能力。

**时间理解（3个维度）**：

10. **动作识别**：要求模型识别视频中显示的动作，测试其捕捉时间动态和动作知识的能力。
11. **动作预测**：基于视频前段内容预测后续动作，需理解上下文和时间推理。
12. **过程理解**：要求模型识别视频中的关键动作并进行时间排序，测试细粒度时间理解能力。


#### 2.3 数据来源


为了构建多样化且具有挑战性的多选题，SEED-Bench 使用了以下数据集：


- **图像数据**：CC3M数据集，经过Tag2Text生成的字幕筛选，仅保留字幕中包含5个以上名词的图像，以确保信息丰富。
- **视频数据**：

**Something-Something-v2 (SSV2)**：包含174个细粒度动作类别，选取1,740个验证集视频用于动作识别。
**Epic-Kitchen 100**：选取138个长视频，包含时间标注的动作标签。
**Breakfast**：用于过程理解任务，提供细粒度动作分割标注。

#### 2.4 多选题生成与验证流程


SEED-Bench 的多选题生成与验证流程（图3）是一个复杂且高质量的管道，具体步骤如下：


**（1）视觉信息提取**：


- **图像**：使用多种基础模型提取视觉信息：

**图像字幕**：BLIP-2生成整体字幕，Tag2Text生成基于实例的描述。
**实例描述**：SAM进行实例分割，Tag2Text提供对象标签，属性检测器提取对象属性，GRiT生成密集字幕。
**文本元素**：PaddleOCR提取图像中的文字。

- **视频**：由于现有模型难以提取细粒度时间信息，直接使用视频数据集的真实标注。

**（2）问题与答案生成**：


- 使用ChatGPT（空间理解）或GPT-4（视觉推理）生成多选题，每题包含四个选项（一个正确答案，三个干扰项）。
- 针对每个评估维度设计特定提示（prompt），确保问题与目标维度相关，且干扰项与正确答案相似以增加挑战性。

**（3）自动过滤**：


- 将生成的问题（不含图像）输入三个强大的LLMs（Vicuna-7B、Flan-T5-XXL、LLaMA-7B），过滤掉能被所有三个模型正确回答的问题（约5.52%），确保问题依赖视觉输入。

**（4）人工验证**：


- 人工标注者验证每个问题的正确答案，并将其分类到对应评估维度。
- 剔除无法基于视觉输入回答、无正确答案或多正确答案的问题，最终生成19K高质量多选题。

#### 2.5 评估策略


SEED-Bench 采用**答案排名策略**评估模型：


- 计算模型对每个选项内容的生成似然度，选择似然度最高的选项作为预测答案。
- 优点：不依赖模型输出特定格式（如“A/B/C/D”），消除选项顺序对结果的影响，相比MMBench的ChatGPT匹配方法（对齐率仅87%）更客观。


### 3. 实验与结果


#### 3.1 评估模型


SEED-Bench 评估了18个模型，包括：


- **3个LLMs**：Flan-T5、Vicuna、LLaMA。
- **12个ImageLLMs**：如BLIP-2、InstructBLIP、LLaVA、MiniGPT-4等。
- **3个VideoLLMs**：VideoChat、Video-ChatGPT、Valley。

#### 3.2 实验结果


实验结果（表3）展示了各模型在空间理解（9个维度平均）、时间理解（3个维度平均）和总体性能的表现。关键发现包括：


1. **整体性能**：

- **InstructBLIP**（基于Vicuna-7B）以53.37%总体准确率排名第一，在8个评估维度上表现最佳。
- **BLIP系列**（BLIP-2、InstructBLIP）在空间和时间理解上均表现优异。
- 大多数MLLMs的总体准确率低于50%，表明当前模型在多模态理解能力上仍有较大提升空间。

2. **空间理解**：

- MLLMs在**场景理解**和**视觉推理**上表现较好（准确率&gt;40%），优于纯LLMs，表明其在全局图像理解和推理方面有优势。
- **空间关系**和**文本识别**是弱项，InstructBLIP在空间关系上仅达40%，其他模型在文本识别上普遍低于40%。

3. **时间理解**：

- **VideoLLMs表现不佳**：VideoLLMs（如Video-ChatGPT、Valley）在动作识别、动作预测和过程理解上的表现甚至不如部分ImageLLMs（如InstructBLIP）。
- **过程理解**难度最高，顶级模型VPGTrans仅比LLaMA高5%，表明细粒度时间推理对当前模型是重大挑战。

4. **模型对比**：

- **InstructBLIP的成功原因**：

使用16M样本的多样化指令微调数据集，覆盖多模态任务（如OCR、时间视觉推理）。
冻结LLM权重进行指令微调，避免灾难性遗忘。

- **弱点**：InstructBLIP在动作识别和过程理解上表现不佳，可能因训练数据分布与测试数据不匹配。
- **VideoLLMs的空间理解**：如VideoChat在实例定位上表现良好（39.98%，排名第四），表明联合训练图像和视频数据未显著降低空间理解能力。

#### 3.3 分析与洞见


- **MLLMs的局限性**：大多数MLLMs在12个维度上的性能有限，尤其在细粒度任务（如空间关系、文本识别、过程理解）上表现较差。
- **全局 vs. 细粒度**：MLLMs在全局图像理解（如场景理解）上表现较好，但在需要细粒度分析的任务上（如空间关系、实例交互）表现较弱。
- **文本识别问题**：除InstructBLIP外，模型在文本识别上的低准确率（&lt;40%）反映了预训练数据集中缺乏丰富的文本元素。
- **时间理解挑战**：VideoLLMs未能在时间理解任务上展现明显优势，表明当前模型在细粒度动作识别和时间推理方面仍需改进。


### 4. 创新点与贡献


- **规模与多样性**：SEED-Bench 是首个包含19K多选题的综合性多模态基准，覆盖12个评估维度，远超现有基准的规模和多样性。
- **高质量数据**：通过自动过滤和人工验证，确保问题和答案的高质量，减少主观性和错误。
- **客观评估**：采用答案排名策略，避免依赖模型的指令跟随能力，提高评估的客观性和效率。
- **全面洞见**：通过评估18个模型，揭示了MLLMs在空间和时间理解上的优势与不足，为未来研究指明方向。
- **持续维护**：计划推出并维护排行榜，为研究社区提供持续评估平台。


### 5. 未来工作


- **扩展评估维度**：增加更多评估维度，覆盖更广泛的多模态任务。
- **改进视频信息提取**：探索自动提取视频时间信息的方法，减少对人工标注的依赖。
- **增强文本识别能力**：开发包含丰富文本元素的预训练数据集，提升模型在文本识别任务上的性能。
- **细粒度时间理解**：针对动作识别、动作预测和过程理解等任务，设计更有效的训练策略和模型架构。


### 6. 总结

《SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension》提出了一种大规模、客观的多模态基准，用于评估MLLMs的生成性理解能力。SEED-Bench 通过19K个人工标注的多选题，覆盖12个空间和时间理解维度，揭示了现有模型的局限性，并为未来多模态模型研究提供了重要洞见。

# 《MULTIBENCH: Multiscale Benchmarks for Multimodal Representation Learning》

由Paul Pu Liang等人于2021年在NeurIPS的Datasets and Benchmarks轨道发表，详细介绍了一个全新的多模态学习基准——MULTIBENCH。本文旨在解决多模态研究中资源有限的问题，特别是在跨领域和模态的泛化能力、训练与推理的复杂性以及对噪声和缺失模态的鲁棒性等方面。

### 1. 背景与动机


#### 多模态学习的定义与重要性


多模态学习（Multimodal Machine Learning）涉及从多种异构数据源（例如语言、图像、视频、音频、时间序列等）中整合信息，以进行预测或生成任务。文章指出，人类感知世界的方式天生是多模态的，涉及视觉、听觉、触觉等多种感官，因此多模态学习在现实世界中有广泛的应用，包括多媒体、情感计算、机器人、金融、医疗和人机交互等领域。然而，多模态学习面临独特的技术挑战，例如如何处理异构数据源的多样性、如何捕获模态之间的对应关系以及如何在实际应用中实现高效性和鲁棒性。


#### 当前多模态研究的局限性


尽管在语言和视觉领域（如图像-文本任务）取得了显著进展，但在其他模态（如时间序列、传感器数据）和领域（如医疗、金融、人机交互）的多模态研究相对较少。此外，现有基准主要关注模型性能，忽视了训练和推理的复杂性以及对噪声和缺失模态的鲁棒性。这些局限性限制了多模态模型在现实世界中的部署，因为实际应用需要平衡性能、复杂性和鲁棒性。


#### MULTIBENCH的目标


为了加速多模态研究并解决上述问题，作者提出了MULTIBENCH，一个系统化、统一的大规模多模态学习基准，旨在：


1. **跨领域和模态的泛化**：通过覆盖15个数据集、10种模态、20个预测任务和6个研究领域，评估模型在不同场景下的泛化能力。
2. **训练与推理的复杂性**：量化多模态模型在时间和空间复杂性上的开销，促进高效模型的设计。
3. **鲁棒性**：评估模型在面对噪声和缺失模态时的表现，模拟现实世界中的数据不完善情况。
4. **标准化与可复现性**：提供端到端的机器学习流水线和20种多模态方法的标准化实现（MULTIZOO），确保研究的可复现性和易用性。


### 2. MULTIBENCH的组成


#### 2.1 数据集


MULTIBENCH包含15个数据集，覆盖6个研究领域（情感计算、医疗、机器人、金融、人机交互、多媒体），涉及10种模态（语言、图像、视频、音频、时间序列、表格数据、力传感器、 proprioception传感器、集合、 optical flow）。这些数据集的规模从小型（690个样本）到大型（147,000个样本），任务类型包括分类、回归等。以下是各领域数据集的概述：


- **情感计算（Affective Computing）**：

**MUSTARD**（690样本，语言+视频+音频）：预测讽刺（sarcasm）。
**CMU-MOSI**（2,199样本，语言+视频+音频）：预测情感（sentiment）。
**UR-FUNNY**（16,514样本，语言+视频+音频）：预测幽默（humor）。
**CMU-MOSEI**（22,777样本，语言+视频+音频）：预测情感和情绪（emotions）。
这些数据集需要模型对语言、视觉和音频的时间序列数据进行融合，解决模态对齐和互补信息的挑战。

- **医疗（Healthcare）**：

**MIMIC**（36,212样本，语言+表格数据）：预测ICU患者的死亡率和ICD-9疾病代码。挑战在于整合时间变化的动态数据和静态人口统计数据。

- **机器人（Robotics）**：

**MuJoCo PUSH**（37,990样本，图像+力传感器+proprioception传感器）：预测机器人推动物体的位置。
**Vision&Touch**（147,000样本，图像+力传感器+proprioception传感器）：预测接触和机器人末端执行器的位置。强调对传感器失效的鲁棒性。

- **金融（Finance）**：

**STOCKS-F&B/STOCKS-HEALTH/STOCKS-TECH**（各5,218样本，语言+时间序列）：使用多只股票的历史价格预测目标股票的价格和波动性。挑战在于处理大量模态（18/63/100）和低信噪比。

- **人机交互（HCI）**：

**ENRICO**（1,460样本，图像+集合）：预测Android应用界面的设计主题，用于UI设计和用户交互建模。

- **多媒体（Multimedia）**：

**AV-MNIST**（样本数未明确，图像+音频）：预测手写数字。
**MM-IMDB**（25,959样本，语言+图像）：预测电影类型（多标签分类）。
**KINETICS400-S/KINETICS400-L**（各20,000样本，语言+音频+optical flow）：预测人类动作。

这些数据集的选择反映了现实世界的多样性和挑战性，例如大规模模态处理、对齐问题和噪声鲁棒性。


#### 2.2 评估协议


MULTIBENCH设计了全面的评估协议，关注以下三个方面：


1. **性能（Performance）**：

- 针对不同任务使用标准化指标，如回归任务的均方误差（MSE）和平均绝对误差（MAE），分类任务的准确率（Accuracy）、F1分数（Micro/Macro F1）和AUPRC。
- 确保跨数据集的性能比较公平且可复现。

2. **复杂性（Complexity）**：

- 量化模型在训练和推理过程中的时间和空间复杂性，包括数据大小（以位为单位）、模型参数数量、训练时间、内存使用量以及CPU/GPU上的推理时间和内存。
- 强调在现实世界中（如移动设备）需要轻量级模型。

3. **鲁棒性（Robustness）**：

- 评估模型在面对模态特定缺陷（如图像翻转、文本拼写错误、音频缩写）和多模态缺陷（如缺失模态或时间序列数据段丢失）时的表现。
- 使用性能-缺陷曲线（performance-imperfection curve）和定量指标（如相对鲁棒性和有效鲁棒性）来评估。

#### 2.3 MULTIZOO：多模态算法工具包


MULTIBENCH附带了一个标准化工具包MULTIZOO，实现了20种多模态方法，覆盖数据预处理、融合范式、优化目标和训练流程。以下是主要模块的概述：


- **数据预处理**：

**WORDALIGN**：针对时间序列数据，将其他模态的信息对齐到文本的单词级别粒度。

- **融合范式（Fusion Paradigms）**：

**早期融合（Early Fusion, EF）**：在输入阶段将模态数据拼接。
**晚期融合（Late Fusion, LF）**：对每个模态应用单独模型，提取特征后再拼接并分类。
**张量融合（Tensor Fusion, TF）**：通过外积捕获模态间高阶交互，但计算成本高。
**低秩张量融合（Low-rank Tensor Fusion, LRTF）**：TF的低秩近似，降低计算复杂性。
**多模态交互（Multiplicative Interactions, MI）**：通过可学习的参数（如矩阵、向量或标量）捕获模态交互，衍生出HyperNetworks、FiLM和Sigmoid units。
**多模态门控单元（Multimodal Gated Units）**：如NL GATE，使用注意力机制动态调整模态表示。
**时间注意力模型（Temporal Attention Models）**：如Transformer，自动对齐时间序列数据并捕获互补信息。

- **优化目标和训练流程**：

包括对比学习（REFNET）、典型相关分析（CCA）、多模态变分自编码器（MVAE）等优化目标，以及梯度混合（GRADBLEND）等训练策略。

MULTIZOO的模块化设计便于新研究者使用、方法组合和结果复现。代码示例显示，只需不到10行代码即可训练一个多模态模型，极大地降低了使用门槛。



### 3. 实验与讨论


#### 3.1 实验设置


作者使用MULTIBENCH加载数据集并测试MULTIZOO中的多模态方法，保持一致的训练循环以控制变量。实验结果在附录中详细列出，代码公开在GitHub上。


#### 3.2 主要发现


实验结果揭示了多模态模型在性能、复杂性和鲁棒性方面的表现和权衡：


1. **标准化带来的益处**：

- 在15个数据集中的9个上，通过应用来自不同研究领域的多模态方法（如GRADBLEND、CCA）取得了新的最优性能，尤其在医疗、金融和人机交互等较少研究的领域。例如，在MIMIC数据集上，外部领域方法将性能从77.9%提升至78.2%；在ENRICO数据集上，性能从47.0%提升至51.0%。
- 标准化使方法跨领域应用变得可能，加速了多模态研究。

2. **跨领域和模态的泛化**：

- **高方差**：许多多模态方法在设计领域（in-domain）表现优异，但在其他领域（out-domain）性能下降。例如，MFAS在多媒体数据集（AV-MNIST、MM-IMDB）上表现良好，但在医疗（MIMIC）上效果不佳；MuLT在情感计算数据集上表现突出，但在金融和机器人领域表现较差。
- **简单方法的优势**：晚期融合（LF）等简单方法在跨领域表现稳定，性能始终高于平均水平，但从未达到最优，表明其是良好的起点但非最佳解决方案。
- **模态限制**：某些方法（如MFAS、CCA）仅适用于2种模态（如图像+文本），而张量融合（TF）和多模态交互（MI）在超过2-3种模态时计算成本高，难以直接应用于大规模模态数据集（如金融领域的18/63/100种模态）。

3. **单模态与多模态的权衡**：

- 单模态模型在许多数据集上表现良好，表明最佳单模态性能已接近多模态模型的水平。多模态模型的进一步提升通常需要2-3倍的参数量。例如，在ENRICO数据集上，多模态模型比最佳单模态模型性能提升4%。

4. **性能与复杂性的权衡**：

- 图3和图24显示，简单融合方法（如EF、LF）在性能和复杂性之间取得了良好平衡，而复杂方法（如MFAS、MuLT）虽然性能略高，但训练时间和参数量显著增加。例如，在金融数据集上，LF-LSTM的训练时间为62秒，参数量为0.005M，而MuLT的训练时间为160秒，参数量为0.125M。
- 复杂方法（如MFAS）在设计领域表现优异，但在新领域适应困难，提示未来需要研究轻量级且泛化的多模态模型。

5. **性能与鲁棒性的权衡**：

- 图4和图25显示，性能较高的模型在相对鲁棒性（relative robustness，面对缺陷时的绝对性能）上通常表现较好，但在有效鲁棒性（effective robustness，性能下降速率）上可能较差。例如，MuLT、CCA和MVAE在初始性能高的情况下，面对噪声的性能下降更快。
- 很少有模型同时在相对鲁棒性和有效鲁棒性上表现优异，表明鲁棒性是多模态研究的未来方向。

6. **领域特定的观察**：

- **金融领域**：数据集（如STOCKS-F&B/HEALTH/TECH）具有大量模态和低信噪比，传统方法（如TF、MuLT）需通过模态聚类减少模态数量，性能略低于预期。GRADBLEND初始性能最佳，但在噪声增加时鲁棒性最差。
- **人机交互（HCI）**：ENRICO数据集上，LF和GRADBLEND比单模态模型更鲁棒，尤其是对图像模态的噪声。复杂方法（如TF、LRTF）因参数过多可能过拟合。
- **多媒体**：AV-MNIST上，MFAS保持最佳性能；MM-IMDB上，CCA和REFNET通过优化目标提升性能；KINETICS数据集上，GRADBLEND在大型分区表现好，但在小型分区表现不佳，提示需要轻量级模型。


### 4. 未来方向


MULTIBENCH计划持续扩展，涵盖更多数据集、模型和评估维度，以推动多模态研究的进步：


1. **数据集扩展**：

- **融合任务**：添加如Hateful Memes（多媒体）、IEMOCAP（情感计算）、CMU-MOSEAS（多语言情感识别）等数据集，关注隐私和公平性。
- **检索任务**：纳入跨模态检索数据集，如基于文本查询图像或视频。
- **问答（QA）**：添加视觉问答（Visual QA）、社交智商（Social IQ）等数据集，扩展到多模态查询（如文本+图像+表格）。
- ** grounding任务**：纳入视觉指代表达式任务，并扩展到其他模态（如语言查询视频/音频）。
- **强化学习（RL）**：添加语言条件RL和多传感器机器人操作数据集，研究交互环境中多模态表示。

2. **模型扩展**：

- MULTIZOO的模块化设计便于添加新单模态编码器和多模态方法。作者维护的阅读列表将定期更新最新方法，并通过社区贡献纳入MULTIZOO。

3. **评估扩展**：

- **不确定性估计**：加入不确定性预测评估（如Uncertainty Toolkit），支持对模型置信度的评估。
- **分布漂移鲁棒性**：添加测试跨领域和子群体泛化的数据分区，纳入单模态分布漂移方法，并设计标准化评估流程。
- **公平性**：通过数据标注、算法设计和评估指标（如个体/群体公平性）研究多模态模型中的社会偏见，特别关注人类中心任务。

4. **社区参与**：

- 通过研讨会（如NAACL 2021、ACL 2020）和CMU多模态课程推广MULTIBENCH，鼓励学生和研究者贡献新数据集和模型。
- 在GitHub上提供任务提案和代码贡献指南，定期审查社区建议，持续更新MULTIBENCH。


### 5. 结论与意义


MULTIBENCH通过提供15个多样化数据集、10种模态、20个任务和6个研究领域的统一基准，填补了多模态研究中的资源空白。其标准化流水线和MULTIZOO工具包降低了研究门槛，促进了可复现性和跨领域方法的应用。实验结果显示，标准化显著提升了9个数据集的性能，揭示了泛化、复杂性和鲁棒性的权衡，为未来研究指明了方向。MULTIBENCH的开放性和持续扩展计划使其成为多模态学习领域的重要资源，将推动更通用、轻量和鲁棒的多模态模型的发展。

# 《MATHVISTA: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts》

### 1. 引言与研究背景


**文章背景**
文章《MATHVISTA: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts》于2024年作为ICLR（International Conference on Learning Representations）的会议论文发表，作者来自加州大学洛杉矶分校（UCLA）、华盛顿大学和微软研究院。该研究聚焦于评估大型语言模型（LLMs）和大型多模态模型（LMMs）在视觉上下文中的数学推理能力，这是一个尚未被系统性研究的领域。数学推理是人类智力的重要体现，需要逻辑思维、领域知识和多步骤推理能力，尤其是在结合视觉信息的场景中，这种能力对于教育、数据分析和科学研究等现实世界的应用至关重要。


**研究动机**
尽管现有数据集（如ChartQA）探索了视觉-语言环境下的数学推理，但这些数据集通常专注于特定任务（如数学应用题）或特定视觉场景（如几何问题或柱状图），缺乏对视觉-语言数学推理的全面评估。此外，通用视觉问答（VQA）数据集中的数学推理问题比例较小，无法充分检验模型在数学框架内的视觉-语言推理能力。因此，文章提出MATHVISTA基准数据集，旨在填补这一空白，系统性地评估AI模型在数学密集且视觉丰富的任务中的表现。


**研究目标**


1. 开发一个综合性的数学推理基准，结合多种数学任务和视觉上下文，反映现实世界的应用需求。
2. 评估12个领先的基础模型（包括LLMs和LMMs）的数学推理能力，揭示其优势与不足。
3. 探索GPT-4V等模型的新兴能力（如自我验证和多轮交互），为未来通用AI代理的发展提供方向。


### 2. MATHVISTA数据集


**数据集概述**
MATHVISTA是一个专为评估视觉上下文中的数学推理能力设计的基准数据集，包含6,141个样本，来源于28个现有的多模态数据集以及3个新创建的数据集（IQTest、FunctionQA、PaperQA）。数据集覆盖了多种任务类型、视觉上下文和数学推理技能，旨在测试模型的精细视觉理解和组合推理能力。


**任务分类与设计原则**
MATHVISTA通过以下分类体系组织任务：


1. **七种数学推理类型**：

- 代数推理（Algebraic Reasoning）
- 算术推理（Arithmetic Reasoning）
- 几何推理（Geometry Reasoning）
- 逻辑推理（Logical Reasoning）
- 数字常识（Numeric Common Sense）
- 科学推理（Scientific Reasoning）
- 统计推理（Statistical Reasoning）

2. **五种主要任务**：

- 图表问答（Figure Question Answering, FQA）：聚焦于图表和统计推理。
- 几何问题求解（Geometry Problem Solving, GPS）：涉及几何学知识。
- 数学应用题（Math Word Problem, MWP）：结合日常场景的算术推理。
- 教材问答（Textbook Question Answering, TQA）：需要科学知识和图形理解。
- 视觉问答（Visual Question Answering, VQA）：通用视觉-语言推理。

3. **多样化的视觉上下文**：包括自然图像、几何图、抽象场景、合成场景、图表、学术插图、表格、函数图、智力测试图形等。

**数据收集**


1. **现有数据集**：

- 从9个数学问答（MathQA）数据集收集2,666个样本，涵盖GPS、MWP和TQA任务。
- 从70多个VQA数据集中筛选出19个包含数学推理的公开数据集，自动过滤后由专家手动标注，最终收集2,739个样本。
- 每个源数据集限制最多400个样本，以确保数据集的平衡性。

2. **新数据集**：

- **IQTest**（228个样本）：针对逻辑推理，基于在线学习平台的智力测试图形，测试归纳推理、抽象思维和模式预测能力。
- **FunctionQA**（400个样本）：聚焦于函数图的代数推理，涉及变量、表达式、方程等。
- **PaperQA**（107个样本）：基于2023年8月Huggingface平台发布的学术论文中的表格、图表等，测试科学推理能力。
- 新数据集由STEM领域的研究生手动标注，并通过两步审查流程（三名审稿人独立标注+团队讨论解决分歧）确保数据质量，标注一致性高达99.2%。

3. **元数据标注**：

- 每个样本附带细粒度的元数据，包括问题类型、答案类型、任务类别、年级水平、视觉上下文和所需推理技能，便于深入分析模型性能。

**数据集统计与发布**


- 总计6,141个样本，其中55.2%为多选题，44.8%为自由回答题。
- 包含5,487个独特图像、4,446个独特问题和1,464个独特答案。
- 数据集分为两个子集：

**testmini**（1,000个样本）：用于模型开发验证或计算资源有限的情况。
**test**（5,141个样本）：用于标准评估，答案标签不公开，通过在线评估平台防止数据污染。

- 数据分布通过采样策略优化，确保testmini与整体数据集的分布接近（KL散度0.008，总变差距离0.035）。


### 3. 实验设计与结果


**实验目标**
通过MATHVISTA评估12个领先基础模型的数学推理能力，包括3个LLMs（ChatGPT、GPT-4、Claude-2）、2个专有LMMs（GPT-4V、Bard）和7个开源LMMs（如IDEFICS-9B、LLaVA等）。实验从量化和质性两个角度分析模型性能，揭示其在视觉-数学推理任务中的优势与不足。


**评估协议**


1. **评估流程**：

- **响应生成**：模型根据任务描述、问题、选项和元数据生成响应。
- **答案提取**：使用GPT-4等LLM提取简短答案，准确率超过99.5%。
- **分数计算**：将提取的答案标准化后计算准确率，作为确定性评估指标。

2. **实验设置**：

- **Text-Only LLMs**：测试零样本和两样本设置，采用链式思考（CoT）和程序化思考（PoT）提示策略。
- **Augmented LLMs**：为LLMs提供外部视觉信息（如Bard生成图像描述、EasyOCR提取文本）。
- **LMMs**：直接处理图像和问题，GPT-4V通过手动评估（因无API访问）。
- **人类基线**：通过Amazon Mechanical Turk招募高中以上学历的标注者，完成5个问题（20分钟内），人类准确率为60.3%。

**主要结果**


1. **整体性能**：

- **GPT-4V**：最佳模型，准确率49.9%，领先第二名Bard（34.8%）15.1%，但仍比人类性能（60.3%）低10.4%。
- **Bard**：多模态模型中次佳，准确率34.8%，仅达人类性能的58%。
- **PoT GPT-4（增强型）**：准确率33.9%，接近Bard，表明外部视觉工具可提升LLM性能。
- **Text-Only LLMs**：最佳为2样本CoT GPT-4，准确率29.2%，显示纯文本模型在视觉任务中的局限性。
- **开源LMMs**：性能普遍低于专有模型，如IDEFICS-9B等。

2. **任务与推理类型分析**（表7）

- **任务表现**：

GPT-4V在GPS（几何问题求解）、TQA（教材问答）和FQA（图表问答）上表现优异，甚至超过人类性能。
在VQA和MWP任务中，模型表现较弱，反映出对复杂视觉上下文和日常场景推理的挑战。

- **推理类型**：

GPT-4V在代数推理（ALG）、科学推理（SCI）和统计推理（STA）上表现强劲，但在逻辑推理（LOG）和数字常识（NUM）上较弱。
Bard的错误主要源于视觉感知和文本推理中的幻觉（Hallucination）以及计算错误。

3. **视觉上下文分析**

- GPT-4V在函数图、几何图、散点图和表格等复杂视觉上下文中的表现优于其他模型（图1）。
- 示例（如图99、图100）显示，GPT-4V能正确解读茎叶图和小提琴图，而其他模型（如LLaVA、Bard）常因误解视觉信息而失败。

**定性分析**


1. **Bard的错误分析**（图4、图5）：

- 44.6%的预测答案和解释均错误，6.8%答案正确但解释部分错误，8.1%答案正确但解释完全错误。
- 49.6%的错误解释涉及幻觉，19.5%源于错误计算，18.6%为错误推理伴随幻觉。
- 示例（图5b）：Bard在几何问题中错误使用余弦定律，而应利用等腰三角形性质。

2. **增强型GPT-4的分析**（图6）：

- 增强型模型依赖外部视觉工具的质量，准确的OCR文本可提升性能（如图6a），但幻觉（如图6b）会干扰推理。


### 4. GPT-4V的深入分析


**性能优势**
GPT-4V的优越性主要归因于其强大的视觉感知和数学推理能力，尤其在代数推理、几何问题和复杂视觉上下文（如表格、函数图）中表现突出。图1显示，GPT-4V在某些任务和视觉上下文中甚至超越人类性能。


**新兴能力**


1. **自我验证（Self-Verification）**（H.5节，图101-105）：

- GPT-4V能检查自身推理过程并纠正错误，例如在图101中验证三角形周长约束，或在图103中调整半径计算。
- 自我验证在复杂视觉或非英语任务（如图105的中文问题）中效果较弱，且不保证正确答案（图104）。
- 其他模型（如Bard，图106）缺乏类似鲁棒的自我验证能力。

2. **自我一致性（Self-Consistency）**（H.6节，图107-112）：

- 通过多次运行生成不同推理路径，选择最频繁的答案，自我一致性可纠正视觉感知错误（图107）、计算错误（图108）和幻觉（图109）。
- 但在复杂视觉上下文（如抽象场景，图110）或关键信息提取失败（图112）时效果有限。

3. **多轮人-AI交互**（H.7节，图113-120）：

- GPT-4V能利用用户提示（如反馈或领域知识）改进答案，例如纠正视觉感知错误（图114）、重新评估推理步骤（图115）或整合复杂上下文（图118）。
- 失败案例包括问题模糊（图119）或抽象形状理解困难（图120）。


### 5. 相关工作与贡献


**相关工作**


- **现有数学推理基准**：如GSM-8K等专注于文本任务，已出现性能饱和，缺乏视觉上下文。
- **VQA数据集**：如Anttol等测试视觉推理，但数学推理问题比例低，覆盖面有限。
- **生成式基础模型**：如GPT-4、Bard等在多任务处理上表现出色，但视觉-数学推理能力未被系统评估。

**MATHVISTA的贡献**


1. **综合性基准**：整合28个现有数据集和3个新数据集，覆盖多种任务、推理类型和视觉上下文。
2. **挑战性设计**：包含高级主题（如大学课程、科学推理），人类准确率仅60.3%，对模型构成挑战。
3. **细粒度评估**：通过元数据分析模型在不同任务、推理类型和视觉上下文中的表现。
4. **新兴能力探索**：首次量化评估GPT-4V，揭示其自我验证、自我一致性和交互能力，为未来研究指明方向。


### 6. 结论与未来方向


**结论**
MATHVISTA是一个开创性的基准数据集，系统性地评估了基础模型在视觉上下文中的数学推理能力。实验表明，GPT-4V以49.9%的准确率领先，但与人类性能（60.3%）仍有10.4%的差距，凸显了视觉-数学推理的挑战性。Bard等模型的错误主要源于幻觉和计算失误，而GPT-4V的自我验证和交互能力为其未来发展提供了潜力。


**未来方向**


1. 开发更强大的多模态模型，整合视觉理解与数学推理能力。
2. 优化自我验证机制，确保在复杂场景和非英语任务中的鲁棒性。
3. 增强模型在多轮交互中的上下文理解能力，处理模糊问题和抽象视觉内容。
4. 扩展MATHVISTA，纳入更多语言和领域特定的视觉-数学任务。


### 7. 总结


《MATHVISTA》通过一个包含6,141个样本的综合性数据集，系统性地评估了12个基础模型在视觉-数学推理任务中的表现。数据集设计科学，覆盖多种任务、推理类型和视觉上下文，揭示了模型在复杂视觉场景中的局限性。GPT-4V以其卓越的视觉感知和数学推理能力脱颖而出，但仍需改进以缩小与人类的性能差距。其新兴能力（如自我验证和多轮交互）为未来通用AI代理的发展提供了重要启示。该研究为AI在数学密集和视觉丰富任务中的应用奠定了基础，具有重要的学术和实践价值。

# 《Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering》

### 1. 研究背景与动机


#### 背景


人工智能（AI）系统的长期目标是实现像人类一样可靠、高效地完成复杂任务。人类在决策时通常会通过显式的**思维链（Chain of Thought, CoT）进行多步推理，并以解释的形式表达出来。然而，当前的深度学习模型（如大型语言模型）通常是黑箱模型**，仅能输出最终答案，缺乏对推理过程的透明性。这种不透明性使得人们难以判断模型是否真正理解任务或能否泛化到新问题上。


科学问题求解是一个需要多模态理解（文本和图像）和多步推理（multi-hop reasoning）的复杂任务，适合用来评估AI系统的推理能力和可解释性。现有的科学问题数据集（如AI2D、DVQA、VLQA等）存在以下局限：


- 缺乏答案的详细注解（尤其是推理过程的解释）。
- 数据规模较小或仅限于单一模态（通常是文本）。
- 主题覆盖范围有限，缺乏领域多样性。

为了解决这些问题，作者提出了**SCIENCEQA**数据集，这是一个包含约21,208个多模态多选题的大型科学问题数据集，涵盖自然科学、社会科学和语言科学三大领域，并为大多数问题提供了详细的**讲座（lecture）**和**解释（explanation）**注解，以揭示推理过程。


#### 研究目标


本文旨在：


1. 构建一个大规模、多模态、领域多样的科学问题数据集（SCIENCEQA），为AI系统的多模态理解和多步推理提供基准。
2. 探索通过**思维链（CoT）**方法增强语言模型的推理能力，使其在回答科学问题时生成类似人类的推理过程（讲座和解释）。
3. 验证CoT方法在**少样本学习（few-shot learning）**和**微调（fine-tuning）**场景下对模型性能的提升，以及是否能帮助模型以更少的数据实现更高的性能。


### 2. SCIENCEQA数据集


#### 数据集概述


SCIENCEQA是一个包含**21,208个多选题**的大型多模态科学问题数据集，来源于小学和高中科学课程，覆盖**1至12年级**的知识点。数据集的特点包括：


- **多模态输入**：问题包含文本和图像上下文，48.7%的问题有图像上下文，48.2%的问题有文本上下文，30.8%的问题同时包含两者，33.9%的问题无上下文。
- **领域多样性**：涵盖**自然科学**（如生物、物理、化学）、**社会科学**（如地理、经济学）和**语言科学**（如修辞手法、写作策略），共26个主题、127个类别和379种技能。
- **注解丰富**：83.9%的问题附有**讲座**（提供背景知识），90.5%的问题附有**解释**（揭示答案的具体推理过程）。
- **数据规模**：相比其他科学问题数据集（如Geometry3K、AI2D等），SCIENCEQA规模更大，问题长度更长（平均12.11词，最大141词），输入来源更多样。

#### 数据分析


- **数据划分**：数据集按60:20:20的比例分为训练集（12,726个样本）、验证集（4,241个样本）和测试集（4,241个样本）。
- **问题长度分布**：相比其他视觉问答（VQA）数据集，SCIENCEQA的问题长度分布更均匀，表明其问题复杂度多样。
- **上下文分析**：66.11%的问题包含至少一种上下文（文本或图像），图像上下文分为自然图像（14.0%）和图表（34.8%），增加了理解难度。
- **年级分布**：主要覆盖3至8年级（约80%），10%来自9至12年级，接近美国大学入学标准化考试的难度。

#### 与现有数据集的比较


与其他科学问题数据集相比，SCIENCEQA具有以下优势：


- **规模更大**：21,208个问题，远超Geometry3K（3,002个）、AI2D（4,563个）等。
- **多模态**：包含文本和图像上下文，而非仅限于单一模态。
- **领域多样**：覆盖自然科学、社会科学和语言科学，而非仅限于自然科学。
- **注解完整**：提供讲座和解释，而其他数据集通常缺乏详细的推理注解。


### 3. 方法与模型


#### 核心方法：思维链（CoT）


**思维链（Chain of Thought, CoT）**是指通过生成一系列中间推理步骤（以讲座和解释的形式）来揭示模型的推理过程，模仿人类的多步推理。SCIENCEQA的讲座提供通用背景知识，解释则针对具体问题说明答案的推理过程。作者设计了两种基于CoT的模型：


1. **UnifiedQA with CoT**：对UnifiedQA模型进行微调，使其不仅输出答案，还生成讲座和解释（格式为QCM→ALE，其中Q=问题，C=上下文，M=选项，A=答案，L=讲座，E=解释）。
2. **GPT-3 with CoT Prompting**：通过少样本提示（few-shot prompting）引导GPT-3生成答案、讲座和解释，提示格式为QCM→ALE，使用训练集中的样本作为上下文示例。

#### 基线模型


- **启发式基线**：

**随机选择**：从选项中随机选择答案，重复三次取平均值。
**人类性能**：通过Amazon Mechanical Turk（AMT）收集人类回答，限定高中文凭以上的工作者，每题由三人回答。

- **零样本和少样本基线**：

使用UnifiedQA和GPT-3，输入格式为QCM→A（仅输出答案）。
图像上下文通过ViT和GPT-2生成图像描述。

- **微调基线**：

视觉问答（VQA）模型（如VisualBERT、Patch-TRM等）：将问题、上下文和选项作为文本输入，图像作为视觉输入，通过线性分类器预测答案。
UnifiedQA：将图像转换为描述后，输入文本信息，输出答案。


### 4. 实验设计与结果


#### 实验设置


- **评估指标**：

答案准确率：VQA基线以多分类问题评估准确率；UnifiedQA和GPT-3生成文本答案后，选择最相似的选项计算准确率。
解释质量：使用自动指标（BLEU-1/4、ROUGE-L、Similarity）评估生成讲座和解释的语义相似性，并通过AMT工作者评估解释的相关性、正确性和完整性。

- **实现细节**：

VQA模型训练50个epoch，学习率为5e-5。
UnifiedQA微调50,000次迭代，每1,000次评估，采用早停策略。
GPT-3使用text-davinci-002引擎，通过在线API运行。

#### 实验结果


1. **VQA基线**：

- VisualBERT表现最佳，平均准确率为61.87%，但在社会科学（SOC）领域表现优于Patch-TRM（+22.39%），表明当前VQA模型在不同领域泛化能力不足。

2. **语言模型**：

- **UnifiedQA**：

零样本（QCM→A）：准确率45.79%，优于随机基线但低于VQA模型。
微调（QCM→A）：准确率70.12%。
微调+CoT（QCM→ALE）：准确率提升3.99%，表明生成讲座和解释增强了推理能力。

- **GPT-3**：

零样本：表现接近最佳VQA基线。
少样本（2-shot，QCM→A）：准确率无明显提升。
少样本+CoT（QCM→ALE）：准确率达75.17%，提升1.20%，生成解释的65.2%达到人类评估的“金标准”（相关、正确、完整）。

3. **解释质量**：

- UnifiedQA (CoT) 在自动指标（BLEU-1/4、ROUGE-L、Similarity）上表现最佳。
- GPT-3 (CoT) 在人类评估（相关性88.5%、正确性78.8%、完整性84.5%）上优于UnifiedQA，表明其生成解释更符合人类判断。

4. **上界分析**：

- 将金标准讲座和解释输入GPT-3（QCMLE*→A），准确率提升至94.13%（+18.96%），表明解释对模型性能有显著帮助。

5. **数据效率**：

- UnifiedQA (CoT) 使用40%的训练数据即可达到无CoT模型的性能，表明CoT帮助模型更高效地学习。

#### 分析


- **提示类型**：QCM→ALE提示（包含讲座和解释）准确率最高，方差最小，模型更稳定。
- **样本数量**：2个上下文示例时，GPT-3 (CoT) 达到峰值性能，更多示例反而降低准确率。
- **动态采样**：按主题、类别或技能动态选择上下文示例，准确率与随机采样差异不大。
- **讲座和解释位置**：先预测答案再生成讲座和解释（QCM→ALE）优于先生成讲座和解释，可能因生成长文本时易中断或超出token限制。
- **错误分析**：

模型失败原因包括：无法理解复杂多模态输入、缺乏特定领域知识、生成不相关/不正确/不完整的解释。
图像描述缺乏细粒度语义，特别是在图表类图像中，导致推理失败。


### 5. 讨论与结论


#### 主要贡献


1. **SCIENCEQA数据集**：首个大规模多模态科学问题数据集，包含21,208个问题，覆盖三大领域，附有讲座和解释注解，为多模态理解和多步推理提供基准。
2. **CoT方法的有效性**：

- 在少样本和微调场景下，CoT显著提升模型性能（GPT-3提升1.20%，UnifiedQA提升3.99%）。
- 生成的解释65.2%达到人类评估金标准。

3. **数据效率**：CoT帮助模型以更少数据（40%）实现相同性能，模仿人类通过解释快速学习的能力。

#### 局限与改进方向


- **模型局限**：当前模型在处理复杂多模态输入和领域特定知识时仍有不足，尤其在社会科学和语言科学领域。
- **解释生成**：部分生成解释不相关、不正确或不完整，需改进生成质量。
- **上界潜力**：通过提供金标准解释，模型性能提升18.96%，表明生成高质量解释是未来优化的方向。

#### 社会影响与应用


- **社会影响**：SCIENCEQA数据来源于教科书，无用户隐私或敏感信息，内容经过审查无不当信息，符合AI为公共利益服务的目标。
- **潜在应用**：可用于开发K-12教育应用（如智能辅导系统），并推动多模态学习、多步推理和通用人工智能的研究。


### 6. 总结


本文通过构建SCIENCEQA数据集和应用CoT方法，显著推进了AI在科学问题求解中的多模态理解和推理能力研究。SCIENCEQA的规模、多样性和详细注解使其成为一个有价值的基准，而CoT方法通过生成讲座和解释，不仅提高了模型性能，还增强了推理过程的可解释性和数据效率。未来工作可以聚焦于改进解释生成质量和提升模型在复杂领域知识上的表现，以进一步接近人类水平的推理能力。

# 《ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning》

论文由Ahmed Masry等人于2022年发表，提出了一种新的大规模ChartQA数据集和相关模型，旨在解决图表问答（Chart Question Answering, ChartQA）任务中的视觉和逻辑推理问题。

### 1. 引言和背景


图表（如柱状图、折线图、饼图）是数据分析和决策中常用的工具。人们在分析图表时，常常会提出需要复杂逻辑推理和数学运算的问题，例如比较值、计算差值或求和。这些问题通常涉及图表的视觉特征（例如颜色、高度、位置等），需要结合视觉和数据信息来回答。然而，现有的图表问答数据集和模型存在以下局限性：


1. **模板化问题**：现有数据集（如FigureQA、DVQA、LEAF-QA等）的问题多基于模板生成，缺乏语言的自然性和多样性。
2. **固定词汇答案**：答案通常来自固定词汇集（如“yes”、“no”或图表轴标签），无法处理需要复杂运算的开放性问题。
3. **图表来源单一**：图表多通过编程工具（如Matplotlib）自动生成，缺乏真实世界图表的多样性。
4. **视觉推理不足**：现有模型（如PlotQA）在处理视觉推理问题时表现不佳，因为它们主要依赖数据表而忽略图表的视觉特征。

为了解决这些问题，论文提出了**ChartQA**，一个包含9,608个人工撰写问题和23,111个基于人类撰写的图表摘要自动生成问题的大规模数据集，覆盖20,882张来自真实世界来源的图表。同时，提出了结合视觉特征和数据表的transformer模型，用于处理复杂的视觉和逻辑推理问题。



### 2. ChartQA数据集


#### 2.1 数据收集与准备


ChartQA数据集从四个真实世界来源收集了图表，以确保视觉风格和主题的多样性：


- **Statista**：涵盖经济、政治、工业等多样化主题的图表。
- **Pew Research**：关注社会、经济、人口趋势和公共舆论，提供多样化的图表。
- **Our World In Data (OWID)**：包含关于全球经济、财政和社会问题的图表。
- **OECD**：提供用于政策制定的数据分析和报告。

对于Pew数据集，仅收集图表图像（无数据表）；其他三个来源则提取了数据表、元数据（如标题、图表类型）、SVG文件和相关文本描述。此外，通过SVG文件提取图表元素的边界框信息，用于训练数据提取模型。


#### 2.2 数据标注


数据集的问答对（QA pairs）通过两种方式生成：


1. **人工撰写QA对（ChartQA-H）**：

- 使用Amazon Mechanical Turk (AMT)进行标注，任务要求工作者针对每张图表提出两种类型的问题：

**组合型问题（Compositional Questions）**：涉及至少两个数学或逻辑运算（如求和、差值、平均值）。
**视觉型问题（Visual Questions）**：涉及图表的视觉属性（如颜色、高度、长度）。

- 每张图表由一名工作者提出两个问题及答案，另一名工作者验证答案。答案完全一致时视为正确，否则通过人工检查确定最终答案。
- 人工标注的答案一致性为61.04%（严格匹配），但考虑拼写错误或词汇变体后一致性提高至78.55%。

2. **机器生成QA对（ChartQA-M）**：

- 由于人工标注成本高，论文利用T5模型（Raffel et al., 2020）从Statista的人工撰写图表摘要中自动生成问题。
- 具体方法：

训练两个T5模型：一个用于从摘要中提取答案，另一个基于答案和摘要生成问题。
使用SQuAD数据集进行预训练，确保生成的问题具有语言多样性。

- 为确保质量，采用启发式方法过滤无效问题（如答案不在图表数据表中的问题），并手动验证了1,250个QA对，确认86.64%的问题完整且可回答。

#### 2.3 数据集统计与分析


- **数据集规模**：

ChartQA-H：4,804张图表，9,608个问题。
ChartQA-M：17,141张图表，23,111个问题。
数据集分为训练、验证和测试集（见表2）。

- **图表类型**：

包括柱状图（Bar）、折线图（Line）和饼图（Pie），柱状图占比最高（见表3）。
柱状图和折线图进一步分为简单（数据表仅两列）和复杂（多列，如堆叠柱状图或多折线图）。

- **问题类型**：

通过分析300个随机选取的人工问题，发现：

**数据检索问题（Data Retrieval）**：13.0%，如“男性认为情人节被高估的百分比是多少？”
**视觉问题（Visual）**：10.7%，如“最右侧浅蓝色柱的值是多少？”
**组合型问题（Compositional）**：43.0%，如“贫困百分比超过11%的年份有多少？”
**视觉与组合型问题（Visual & Compositional）**：33.3%，如“从左数第二和第三个年龄组中，哪个意见差异最大？”


视觉问题中，常见引用包括颜色（44.7%）、长度（40.15%）、大小（11.36%）和位置（8.33%）等。

- **语言统计**：

ChartQA-H：问题和答案分别有6,150和4,319个唯一词汇。
ChartQA-M：问题和答案分别有12,379和11,979个唯一词汇。
问题具有多样化的句法结构，有时包含非正式语言或拼写错误，增加了任务的挑战性。

- **主题分布**：

数据集覆盖政治、经济、健康、社会等多个主题，政治在Pew数据集中占比最高（45.4%）。


### 3. 方法


论文提出了一种结合视觉特征和数据表的ChartQA系统，分为两个问题设置：


1. **提供金标准数据表**：假设图表的数据表已知。
2. **自动提取数据表**：从图表图像中提取数据表，适用于真实世界场景。

#### 3.1 数据提取


- **ChartOCR扩展**：

基于ChartOCR（Luo et al., 2021），通过关键点检测网络定位图表元素（如柱、线、饼图分段）及其文本标签。
使用CRAFT模型（Baek et al., 2019）识别文本，并通过颜色和位置信息将数据值与标签关联，生成结构化数据表。
例如，柱状图中将数据值（如“17.13”）与x轴标签（如“Snapchat”）或图例标签（如“2016”）匹配。

#### 3.2 模型


论文提出了两种基于transformer的模型，结合视觉特征和数据表：


1. **T5**：

- 一个编码器-解码器模型，将问题和展平的数据表作为输入，格式为：“Question: 问题文本 Table: 展平表格文本”。
- 通过生成答案直接解决ChartQA任务。

2. **VL-T5**：

- T5的视觉-语言扩展，额外输入图表的视觉特征（通过Mask R-CNN提取的15种对象，如柱、线、图例等）。
- 视觉特征通过ResNet-101骨干网络提取，固定长度为36（通过零填充处理图表元素数量变化）。

3. **TAPAS**：

- 基于BERT的模型，通过添加行列位置嵌入编码表格结构。
- 输入格式为：[CLS] 问题文本 [SEP] 展平表格文本，输出包括聚合操作（如COUNT、SUM）和单元格选择。

4. **VisionTaPas**（论文提出的新模型）：

- 扩展TAPAS以处理图表图像，包含三个组件：

**视觉变换器（ViT）**：编码图表图像，生成图像特征。
**TAPAS编码器**：编码问题和数据表。
**跨模态编码器**：通过多头跨注意力机制融合视觉和文本-表格特征。

- 扩展操作头，支持减法和比率操作，并通过启发式方法生成单元格选择的监督信号。


### 4. 实验与评估


#### 4.1 数据集与基线


- **评估数据集**：FigureQA、DVQA、PlotQA和ChartQA。
- **基线模型**：

**PReFIL**：基于分类的视觉QA模型，融合问题和图像特征。
**PlotQA***：重新实现的PlotQA模型，使用论文的数据提取方法。

- **评估指标**：

数值答案：若预测答案在金标准答案的±5%范围内视为正确。
非数值答案：要求完全匹配。

#### 4.2 实验结果


- **在现有数据集上的表现**：

当提供金标准数据表时，VisionTaPas和VL-T5接近完美表现。
在DVQA（全自动设置）和PlotQA V1上，VisionTaPas和VL-T5分别取得最优结果（如DVQA测试集上VisionTaPas达94.54%，比PReFIL高14.5%）。
VisionTaPas对OCR噪声更鲁棒，性能下降仅0.92%（PReFIL下降16.49%）。
在PlotQA上，VisionTaPas和VL-T5显著优于PlotQA模型，因支持差值和比率操作。
在FigureQA上，TAPAS表现最佳，因问题主要依赖数据表。

- **在ChartQA上的表现**：

VisionTaPas在两种设置（提供/不提供金标准数据表）下均取得最优性能（45.52%）。
PReFIL表现较差（4.8%），因其分类方法不适于开放词汇问题。
VL-T5未显著优于T5，可能因ChartQA中的视觉问题涉及多元素引用，VL-T5难以捕捉。
按图表类型（表10）：

柱状图：VisionTaPas 49.80%，VL-T5 45.82%。
折线图：VisionTaPas 38.20%，VL-T5 35.40%。
饼图：VisionTaPas 24.41%，VL-T5 25.00%。


按问题类型（表11，基于200个样本）：

数据检索：VisionTaPas 60.00%，VL-T5 50.00%。
视觉问题：VisionTaPas 29.74%，VL-T5 19.14%。
组合型问题：VisionTaPas 34.80%，VL-T5 24.41%。
视觉与组合型：VisionTaPas 36.21%，VL-T5 21.62%。

#### 4.3 定性分析与挑战


- **逻辑推理与嵌套操作**：模型在处理嵌套操作（如先求和再求差）时表现不佳。例如，Q1要求计算“印尼和爱尔兰股份总和与毛里塔尼亚股份的差值”，模型仅输出两数之差。
- **输入表示**：当前模型分别处理数据表和视觉特征，难以捕捉图表结构。复杂的视觉组合问题需要多阶段推理，现有表示方法不足。
- **计算机视觉挑战**：当不提供金标准数据表时，性能下降，表明数据提取准确性需改进。当前方法依赖模块化深学习和规则，易出错。


### 5. 贡献与结论


- **主要贡献**：

**ChartQA数据集**：首个结合人工撰写和机器生成问题的大型数据集，覆盖真实世界图表，强调视觉和逻辑推理。
**新方法**：提出VisionTaPas模型，结合视觉特征和数据表，显著提升了复杂推理问题的性能。
**深入评估**：在多个数据集上验证了模型的有效性，并揭示了视觉和逻辑推理的挑战。

- **结论**：

ChartQA数据集为图表问答提供了更真实的测试场景，问题语言丰富且多样。
尽管VisionTaPas等模型取得最优结果，但仍面临嵌套操作、复杂视觉推理和数据提取准确性等挑战。
论文公开了数据集和代码（[https://github.com/vis-nlp/ChartQA），为未来研究奠定了基础。](https://github.com/vis-nlp/ChartQA%EF%BC%89%EF%BC%8C%E4%B8%BA%E6%9C%AA%E6%9D%A5%E7%A0%94%E7%A9%B6%E5%A5%A0%E5%AE%9A%E4%BA%86%E5%9F%BA%E7%A1%80%E3%80%82)


### 6. 伦理考虑


- **数据来源**：确保遵守Statista、Pew、OECD和OWID的公开数据使用条款。
- **标注者报酬**：基于美国最低工资（7.25美元/小时），每任务支付0.6美元，任务耗时3-5分钟。
- **隐私保护**：所有标注者数据匿名化。
- **模型滥用风险**：模型可能被用于误导公众，需注意输出准确性。


### 7. 未来工作


- **改进嵌套操作**：通过顺序训练（如Cho et al., 2018）增强模型处理复杂逻辑推理的能力。
- **优化输入表示**：开发语义图表示（Teney et al., 2017）以捕捉问题、图表对象和数据值的关系。
- **提升数据提取**：探索端到端深度学习方法，改进真实世界图表的数据提取准确性。
- **更好的评估指标**：开发考虑文本标签的图表数据提取评估指标。


### 8. 总结


《ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning》通过引入一个真实世界图表数据集和结合视觉与数据表的新模型，为图表问答任务提供了重要贡献。数据集的多样性和复杂性为研究视觉与逻辑推理提供了挑战性平台，尽管当前模型在某些场景下仍有局限性。未来通过改进模型架构和数据提取技术，有望进一步提升ChartQA任务的性能。

# 《DocVQA: A Dataset for VQA on Document Images》

### 1. 引言 (Introduction)


文章提出了一种新的视觉问答（Visual Question Answering, VQA）数据集 **DocVQA**，专门针对文档图像，包含 **12,767 张文档图像** 和 **50,000 个问题**。与传统的文档分析与识别（Document Analysis and Recognition, DAR）任务（如字符识别、表格提取、键值对提取）不同，DocVQA 强调通过自然语言问题驱动的高层次文档理解，提出了一种“目的驱动”的研究视角。


- **核心目标**：DocVQA 要求系统不仅能提取文档中的文本信息（包括手写、打印或打字文本），还需要理解文档的**布局结构**（如页面结构、表格、表单）、**非文本元素**（如标记、复选框、分隔符）和**视觉样式**（如字体、颜色、加粗等），以回答开放式问题。
- **与通用 VQA 和场景文本 VQA 的区别**：文档图像具有高密度的语义信息，包含复杂的视觉线索和隐含的书写规范，答案空间是开放的，无法局限于固定词表。
- **数据集特点**：

包含多种文档类型（如表格、表单、图表等），来源广泛，覆盖多个行业（烟草、食品、药品、化石燃料和化学）。
问题需要结合文档的文本和结构信息来回答，强调综合理解能力。
提供了一个公共平台（docvqa.org），包括数据集、代码和排行榜。


### 2. 相关工作 (Related Datasets and Tasks)


文章将 DocVQA 与现有的机器阅读理解（Machine Reading Comprehension, MRC）、开放域问答（Open-domain QA）以及视觉问答（VQA）任务进行对比，突出了其独特性。


- **MRC 和开放域 QA**：

MRC 任务（如 SQuAD 1.1、NewsQA）通常基于给定的文本段落提取答案，而 DocVQA 的上下文是文档图像，增加了视觉和结构理解的复杂性。
开放域 QA（如 MS MARCO）需要从大规模语料库（如维基百科）中检索答案，而 DocVQA 直接基于图像内容。
最近基于 Transformer 的预训练模型（如 BERT、XLNet）在 MRC 任务上表现优异，但这些模型处理的是纯文本输入，无法直接应用于文档图像。

- **视觉问答 (VQA)**：

通用 VQA（如 VQA 2.0）关注自然图像，答案通常来自固定词表。
场景文本 VQA（如 ST-VQA、TextVQA）需要理解图像中的文本，但图像通常是现实世界的场景，文本密度较低。
特定领域的 VQA 数据集（如 DVQA、FigureQA）专注于图表或书封面，问题基于模板生成，缺乏多样性。
DocVQA 的文档图像包含更多的文本和结构元素，问题更开放，挑战更大。

- **DocVQA 的独特性**：

包含多种文档类型（表格、表单、图表等），问题基于真实文档而非模板生成。
强调文档的布局和结构理解，答案通常是开放式的，需从图像中提取。


### 3. DocVQA 数据集


#### 3.1 数据收集


- **文档图像来源**：

数据集中的图像来自 **UCSF 行业文档库**，涵盖 6,071 份文档，时间跨度从 1900 年到 2018 年。
文档类型多样，包括手写、打印、打字和数字化文档，覆盖烟草、食品、药品、化石燃料和化学五个行业。
优先选择包含表格、表单、列表和图表的页面，尽量避免仅包含纯文本的页面，以增加任务的复杂性。
图像质量控制：尽量减少二值化图像，以避免图像质量对 VQA 性能的限制。

- **问题和答案的收集**：

通过基于 Web 的标注工具，由远程工作者完成三阶段标注：

**第一阶段**：工作者为每张文档图像定义最多 10 个问题-答案对，答案需直接从文档中提取，确保问题基于图像中的文本。
**第二阶段**：验证阶段，另一组工作者回答第一阶段的问题，并为问题分配一个或多个问题类型（如表格、表单、布局等）。可标记不合适的问题（如语言问题或歧义）。
**第三阶段**：由作者审查第一阶段和第二阶段答案不匹配的问题，编辑或剔除不合格的问题。


问题设计为**提取式问答**，类似于 NLP 中的 SQuAD 和 ST-VQA，答案直接从文档图像中提取。

#### 3.2 数据统计与分析


- **数据集规模**：

共 50,000 个问题，基于 12,767 张图像，分为训练（80%）、验证（10%）和测试（10%）三个部分。
训练集：39,463 个问题，10,194 张图像；验证集：5,349 个问题，1,286 张图像；测试集：5,188 个问题，1,287 张图像。

- **问题类型**：

问题分为 9 类（见 Figure 3），根据回答所需的信息来源分类：

**表格/列表**：需要理解表格或列表结构。
**表单**：基于键值对信息。
**布局**：需要理解文档的页面结构（如标题、页眉）。
**纯文本**：基于句子或段落信息。
**手写**：基于手写文本。
**其他类型**：如图表、照片等。


问题可以属于多个类型，增加了任务的复杂性。

- **统计分析**：

**问题长度**：平均问题长度为 8.12 个词，仅次于 SQuAD 1.1。70.72% 的问题是唯一的，常见问题包括询问日期、标题和页码（见 Figure 4a、4d）。
**答案长度**：平均答案长度为 2.17 个词，63.2% 的答案是唯一的。常见答案多为数字（报表、发票常见）或命名实体（如人名、机构名、月份）（见 Figure 4b、4c、4e）。
**文本密度**：DocVQA 图像平均包含 182.75 个文本标记，远高于 SQuAD 1.1（117.23 个词）和场景文本 VQA 数据集（不超过 13 个词），表明文档图像的信息密度更高（见 Figure 4f）。
**词云分析**（见 Figure 5）：答案中的常见词多为命名实体（如人名、月份），OCR 提取的常见词与答案高度重叠，表明答案通常直接来自文档文本。


### 4. 基线方法 (Baselines)


文章评估了多种基线方法，包括启发式方法、上界估计和训练模型。


#### 4.1 启发式方法与上界


- **启发式方法**：

随机答案：从训练集答案中随机选择，准确率接近 0%。
随机 OCR 标记：从文档图像的 OCR 标记中随机选择，准确率约 0.5%。
最长 OCR 标记：选择文档中最长的 OCR 标记，准确率接近 0%。
最常见答案：选择训练集中最常见的答案，准确率约 0.9%。
这些方法表现极差，表明简单猜测无法解决 DocVQA 任务。

- **上界估计**：

**词汇表上界 (Vocab UB)**：基于训练集中出现超过一次的答案词汇表，准确率为 31.31%（验证集）/ 33.78%（测试集）。
**OCR 子串上界 (OCR substring UB)**：假设答案是 OCR 标记序列的子串，准确率为 85.64%（验证集）/ 87.00%（测试集）。但子串匹配可能包含错误（如“2”匹配“2020”中的“2”）。
**OCR 子序列上界 (OCR subsequence UB)**：假设答案是 OCR 标记序列的子序列，准确率为 76.37%（验证集）/ 77.00%（测试集）。

#### 4.2 VQA 模型


测试了两种能处理图像文本的 VQA 模型：**LoRRA** 和 **M4C**。


- **LoRRA**：

基于自底向上和自顶向下的注意力机制，结合图像特征（ResNet152 预训练）和 OCR 标记嵌入（FastText）。
使用固定词汇表（训练集答案）和动态词汇表（图像中的 OCR 标记）。
性能较差（测试集 ANLS 0.112，准确率 7.63%），移除固定词汇表后性能骤降，表明模型高度依赖词汇表。
增加动态词汇表大小（50 到 500）效果不明显，表明模型难以处理文档图像中的大量文本。

- **M4C**：

使用多模态 Transformer 和迭代答案预测，结合 BERT 问题嵌入、Faster R-CNN 图像特征、FastText 和 PHOC 的 OCR 标记表示。
性能优于 LoRRA，测试集最佳 ANLS 为 0.391，准确率为 24.81%（动态词汇表大小为 500 时）。
移除对象特征后性能略有提升，表明文档图像中对象检测的特征作用有限。
更大的动态词汇表（150、300、500）显著提高性能，表明 M4C 能更好地利用文档中的大量文本。

#### 4.3 阅读理解模型


测试了基于 BERT 的提取式问答模型，输入为序列化的 OCR 标记。


- **模型变体**：

bert-base：基础模型，微调后测试集 ANLS 0.574，准确率 47.6%。
bert-large：更大模型，微调后测试集 ANLS 0.610，准确率 51.08%。
bert-large-squad：在 SQuAD 1.1 上预训练并微调，微调后测试集 ANLS 0.665，准确率 55.77%，表现最佳。

- **微调设置**：

bert-base：在 2 个 GPU 上微调 2 轮，批大小 32，学习率 5e-05。
bert-large 和 bert-large-squad：在 4 个 GPU 上微调 6 轮，批大小 8，学习率 2e-05。


### 5. 实验 (Experiments)


#### 5.1 评估指标


- **准确率 (Accuracy)**：预测答案与任一目标答案完全匹配的百分比。严格但对 OCR 错误敏感。
- **平均归一化 Levenshtein 相似度 (ANLS)**：考虑预测答案与目标答案的 Levenshtein 距离，减轻 OCR 错误的影响，适合文档 VQA 任务。

#### 5.2 实验设置


- **人类表现**：通过志愿者收集测试集答案，准确率为 94.36%，ANLS 为 0.981，表明人类在文档理解上表现优异。
- **OCR 提取**：使用商业 OCR 工具提取文档图像中的文本标记。
- **模型实现**：LoRRA 和 M4C 使用 MMF 框架的官方实现，BERT 使用 Transformers 库的预训练模型。

#### 5.3 实验结果


- **启发式和上界**（见 Table 1）：

启发式方法准确率低于 1%，表明任务难度高。
OCR 子串上界（87.00%）和子序列上界（77.00%）表明大部分答案可从 OCR 标记中提取，但需要准确的文本和结构理解。

- **VQA 模型**（见 Table 2）：

LoRRA 性能较差（最佳准确率 7.63%），对固定词汇表依赖性强。
M4C 表现更好（最佳准确率 24.81%），动态词汇表大小增加显著提高性能。
对象特征对文档 VQA 的贡献有限，可能因文档图像缺乏传统意义上的“对象”。

- **BERT 模型**（见 Table 3）：

bert-large-squad 表现最佳（测试集准确率 55.77%，ANLS 0.665），表明阅读理解模型在序列化 OCR 输入上的优势。
与人类表现（94.36%）相比，仍有较大差距，尤其在需要布局理解的问题上。

- **定性分析**（见 Figure C.1-C.5）：

BERT 在非阅读理解类问题（如布局问题）上表现较差。
M4C 在基于图片或标志的问题上优于 BERT，类似场景文本 VQA 任务。
两个模型在类似问题上表现不一致，缺乏推理能力。
在图表或图形问题上，两个模型均失败，表明需要更强的结构理解。
OCR 错误（如日期分割、拼写错误）显著影响模型性能。


### 6. 意义与挑战


- **意义**：

**数据集多样性**：DocVQA 提供了一个大规模、多样化的文档图像数据集，覆盖多种行业和文档类型，填补了文档 VQA 领域的空白。
**任务复杂性**：强调文档的布局、结构和非文本元素的理解，推动了“目的驱动”的文档分析研究。
**开放性**：开放的答案空间和问题类型为模型设计提出了更高要求，促进了多模态融合和复杂推理的研究。

- **挑战**：

**模型性能差距**：最佳模型（BERT-large-squad）准确率仅为 55.77%，远低于人类表现（94.36%），表明现有模型在文档结构理解和复杂推理上不足。
**OCR 错误**：OCR 的不准确性（如分割错误、拼写错误）直接影响答案提取。
**布局和非文本元素**：现有模型难以处理需要布局或图形理解的问题，如表格、图表或组织结构图。


### 7. 结论


DocVQA 数据集为文档图像上的视觉问答任务提供了一个全新的研究平台，强调了文档的综合理解能力。通过引入 50,000 个问题和 12,767 张图像，数据集展示了文档 VQA 的复杂性和多样性。基线实验表明，现有 VQA 和阅读理解模型在处理文档图像时仍有较大改进空间，特别是在布局理解和处理 OCR 错误方面。未来研究可以探索更强的多模态模型，结合更精确的 OCR 技术和结构化信息提取方法，以缩小与人类表现的差距。



### 8. 补充内容


- **数据集访问**：数据集、代码和排行榜可在 docvqa.org 获取。
- **标注过程可视化**（见 Figure A.1-A.3）：展示了三阶段标注流程，确保数据质量。
- **示例问题**（见 Figure B.1-B.5、C.1-C.5）：提供了不同类型问题的直观示例，突出了任务的多样性和挑战。


### 总结


《DocVQA: A Dataset for VQA on Document Images》通过引入一个大规模、多样化的文档图像数据集，填补了文档 VQA 领域的空白。文章详细描述了数据集的构建、统计分析、基线方法和实验结果，突出了文档 VQA 的独特挑战，如高密度文本、复杂布局和开放答案空间。实验表明，现有模型在文档结构理解和处理 OCR 错误方面仍有不足，为未来的多模态研究提供了明确方向。

# 《Towards VQA Models That Can Read》

由Amanpreet Singh等人撰写，发表于2019年，来自Facebook AI Research和Georgia Institute of Technology的研究团队。文章聚焦于视觉问答（Visual Question Answering, VQA）模型的一个关键短板：现有VQA模型无法有效读取和推理图像中的文本内容，而这正是视障用户在实际应用中经常提出的需求。文章提出了一个新的数据集TextVQA和一个新颖的模型架构LoRRA（Look, Read, Reason & Answer），旨在解决这一问题。

### 1. 引言与研究背景


#### 背景


视觉问答（VQA）是一个结合计算机视觉和自然语言处理的任务，要求模型根据图像和问题生成正确的答案。近年来，VQA领域取得了显著进展，但现有模型在处理需要读取图像中文字的问题时表现不佳。例如，视障用户常问“我的烤箱温度是多少？”或“这是什么面额的钞票？”，这些问题需要模型识别图像中的文本并进行推理。然而，现有VQA模型（如CVPR VQA挑战赛的顶级模型）在处理这类问题时几乎完全失败。


文章指出，这种失败的原因在于现有VQA模型通常是单一的深度神经网络，缺乏专门的模块来处理文字识别和推理任务。模型需要完成以下复杂步骤：


- 识别问题是否涉及文本（如问题包含“say”或“read”）。
- 检测图像中包含文字的区域（如“15:20”或“500”）。
- 将这些区域的像素表示转换为符号或语义表示。
- 结合文本和图像内容进行联合推理（如根据“飞机尾部的星星附近”定位文字）。
- 决定答案是直接复制文本（如“16”）还是需要基于文本推理（如回答“jet”）。

这些任务的复杂性使得单一网络难以通过端到端的训练自动学会所有这些能力。因此，文章提出通过引入光学字符识别（OCR）模块等专门组件，为VQA模型注入归纳偏见（inductive biases），以增强其文本处理能力。


#### 研究动机


文章强调，文本相关的VQA问题是视障用户在实际场景中的主要需求之一。VizWiz研究表明，视障用户提出的问题中有高达21%涉及读取和推理图像中的文本。然而，现有VQA数据集（如VQA 2.0）中这类问题比例较小，且VizWiz数据集由于58%的问题“无法回答”，规模不足以支持系统性研究。因此，开发一个专门的TextVQA数据集和模型成为当务之急。



### 2. 主要贡献


文章的贡献主要包括以下三点：


1. **TextVQA数据集**：

- 引入了一个新的数据集TextVQA，包含45,336个问题，基于28,408张来自Open Images数据集的图像。这些图像来自包含文本的类别（如广告牌、交通标志、白板等）。
- 每个问题-图像对由人类提供10个地面真相答案，确保答案的多样性和可靠性。
- 问题设计要求模型必须读取和推理图像中的文本才能回答，例如“飞机尾部星星附近的文字是什么？”或“哪个品牌是这些蜡笔？”。

2. **LoRRA模型**：

- 提出了一种新颖的VQA模型架构Look, Read, Reason & Answer（LoRRA），通过集成OCR模块，显式地处理图像中的文本。
- LoRRA不仅关注图像中的对象提议（object proposals），还关注包含文本的区域，并使用OCR识别的文本内容进行推理。
- 模型包含一个“复制机制”（copy mechanism），允许从OCR输出的文本中直接选择答案，或基于文本推理出固定答案空间中的答案。

3. **性能提升**：

- LoRRA在TextVQA数据集上显著优于现有的VQA模型（如Pythia和BAN），验证了其在文本相关问题上的有效性。
- 此外，LoRRA在VQA 2.0数据集上的表现也有所提升，表明其在通用VQA任务中也能利用文本信息。


### 3. TextVQA数据集


#### 数据集构建


TextVQA数据集旨在填补现有VQA数据集在文本相关问题上的空白。其构建过程包括以下步骤：


1. **图像选择**：

- 从Open Images v3数据集（包含大量自然场景图像）中选择图像，重点关注可能包含文本的类别，如广告牌、交通标志和白板。
- 使用Rosetta OCR系统对每个类别的100张随机图像进行文本检测，计算平均文本框数量，并根据此权重采样图像。
- 最终筛选出28,408张含文本图像，分为训练集、验证集和测试集（无图像重叠）。

2. **问题和答案收集**：

- **第一阶段**：通过众包平台，让标注者识别不含文本的图像并剔除。
- **第二阶段**：为每张图像收集1-2个需要读取文本才能回答的问题。要求问题明确依赖图像中的文本，且答案可能需要推理而非直接复制文本。
- **第三阶段**：为每个问题收集10个答案，并提供质量控制选项（如“图像无文本”“非问题”“无需文本即可回答”“无法回答”）以过滤低质量数据。
- 使用已知正确答案的手工问题进一步筛选不可靠的标注者。

#### 数据集统计与分析


- **问题多样性**：

TextVQA包含45,336个问题，其中83.6%（37,912）是唯一的，表明问题多样性高。
平均问题长度为7.18个词，高于VQA 2.0（6.29）和VizWiz（6.68），因为问题通常需要指定文本位置以消除歧义。
问题多以“what”开头（见图4），常见主题包括时间、品牌、城市、名称等。

- **答案多样性**：

包含26,263个唯一多数答案（占49.2%），远高于VQA 2.0（3.4%）和VizWiz（22.8%），表明答案空间多样，难以用固定词汇表覆盖。
平均答案长度为1.58个词，少数情况下答案为长段落K-12或引文。
“yes/no”问题仅占5.55%，且“yes”仅为4.71%的问题的多数答案，表明数据集不偏向简单答案。

- **OCR统计**：

图像平均包含3.14个OCR文本框，81%的图像包含2个或更多文本框。
Rosetta OCR系统偶尔无法检测文本（约1.5k图像），但这些图像经人工验证确实含文本。


### 4. LoRRA模型架构


#### 模型概述


LoRRA（Look, Read, Reason & Answer）是一个模块化的VQA模型，包含三个主要组件：


1. **VQA组件**：处理图像和问题的视觉和语言特征，生成联合表示。
2. **阅读组件**：利用OCR模块识别图像中的文本，并将其嵌入模型推理过程。
3. **回答模块**：决定答案是来自固定答案空间还是直接复制OCR文本。

#### 详细架构


1. **VQA组件**：

- **问题编码**：使用预训练的GloVe词嵌入和LSTM（带自注意力机制）生成问题嵌入 $f_Q(q)$。
- **图像特征**：结合基于网格（ResNet-152的res-5c层）和基于区域（Faster-RCNN的fc6层，训练于Visual Genome）的特征 $f_I(v)$。
- **注意力机制**：使用问题嵌入对图像特征进行注意力加权，生成视觉-问题联合特征 $f_{VQA}(v, q)$。

2. **阅读组件**：

- 使用Rosetta OCR系统提取图像中的文本词 $s_1, s_2, \ldots, s_M$，并通过FastText嵌入生成文本特征 $f_O(s)$。
- 类似VQA组件，使用注意力机制生成OCR-问题联合特征 $f_{OCR}(s, q)$。
- 保留OCR token的注意力权重以保持顺序信息。

3. **回答模块**：

- 扩展答案空间，包含固定词汇表（大小为N）和动态OCR token（大小为M），共预测 $N+M$ 个答案的概率。
- 使用“复制机制”决定是选择固定答案空间中的答案，还是直接复制OCR token。
- 使用二元交叉熵损失（而非softmax）训练，以允许答案同时出现在固定词汇表和O CR token中。

#### 实现细节


- 基于Pythia v0.1（2018年VQA挑战赛冠军）改进为Pythia v0.3，调整了问题词汇表大小和隐藏层维度等超参数。
- 使用AdaMax优化器，批量大小为128，训练24,000次迭代，学习率从5e-2逐步降低到5e-4。
- 最大问题长度为14，最大OCR token数为50，短于此长度则填充。


### 5. 实验与结果


#### 实验设置


- 数据集划分：训练集34,602个问题，验证集5,000个，测试集5,734个。
- 答案词汇表：小词汇表（SA，3,996个答案，出现至少两次）和大词汇表（LA，8,000个最常见答案）。
- 评估指标：使用VQA准确率指标，验证准确率取5次不同种子运行的平均值。

#### 基线与上界


- **上界**：

**OCR UB**：若答案可直接从OCR token构建，准确率为37.12%（验证集）。
**LA UB**：若答案在LA中，准确率为48.46%。
**LA+OCR UB**：结合两者，准确率为67.56%，表明仍有改进空间。

- **启发式基线**：

随机选择前100个答案（Rand 100）：0.22%。
加权随机选择（Wt. Rand 100）：0.27%。
总是预测“yes”（Majority Ans）：4.48%。
随机选择OCR token（Random OCR）：7.72%。
选择最常见OCR token（OCR Max）：9.76%。

#### 训练模型基线


- **仅问题（Q）**：仅使用问题特征，准确率为8.09%。
- **仅图像（I）**：仅使用图像特征，准确率为6.29%。
- **I+Q**（Pythia v0.3和BAN）：分别为13.04%和12.3%，显示现有VQA模型在TextVQA上的局限性。
- **Pythia+O**：加入OCR特征，准确率提升至18.35%。
- **Pythia+O+C**：加入复制机制，准确率为20.06%。

#### LoRRA结果


- **Pythia+LoRRA (LA)**：验证集准确率为26.25%，测试集为27.63%，优于所有基线。
- **Pythia+LoRRA (SA)**：使用小词汇表，准确率进一步提升至26.56%。
- **BAN+LoRRA**：准确率从12.3%提升至18.41%。
- 在VQA 2.0数据集上，Pythia+LoRRA从68.71%提升至69.21%，表明LoRRA对通用VQA任务也有帮助。

#### 分析


- LoRRA在44.9%的验证集问题中选择复制OCR token，55.1%选择固定答案空间。
- 对于答案在OCR token中的30.6%问题，LoRRA正确率为57.5%；对于答案在SA中的48%问题，正确率为38%。
- 模型在处理时间、日期、品牌、城市等问题时表现较好，但对复杂推理或OCR质量不佳时会出错。
- 模型对城市相关问题有“New York”偏见，且在多token答案上受限于单一token复制机制。


### 6. 结论与未来方向


#### 结论


- **TextVQA数据集**：为研究文本相关的VQA问题提供了系统性支持，填补了现有数据集的空白。
- **LoRRA模型**：通过集成OCR和复制机制，显著提升了文本相关问题的回答能力，并在通用VQA任务中也有改进。
- **性能差距**：LoRRA在TextVQA上的准确率（26.56%）远低于人类表现（85.01%）和上界（67.56%），表明仍有改进空间。

#### 未来工作


- 改进OCR系统，处理旋转、模糊或部分遮挡的文本。
- 扩展复制机制，支持多token答案组合。
- 探索更复杂的推理机制，提升模型对文本和图像的联合理解能力。


### 7. 附录分析


#### OCR与答案空间分析


- 44.9%的答案来自OCR token，表明模型对图像文本的依赖。
- OCR答案完全正确率为27%，部分正确率为11%，提示可通过n-gram或拼写纠正改进。
- 81%的图像包含2个或更多OCR token，LoRRA在这些图像上的复制正确率为24.3%。

#### TextVQA示例


- 图7展示了LoRRA在时间、品牌、空间推理等任务上的表现，以及OCR质量对结果的影响。
- 模型在颜色、空间关系和简单大小/形状推理上表现良好，但在复杂问题或OCR失败时易出错。

#### 数据收集界面


- 图8-11展示了三阶段数据收集流程，确保了数据质量和问题设计的针对性。


### 总结

这篇文章通过引入TextVQA数据集和LoRRA模型，为VQA模型赋予了读取和推理图像文本的能力，填补了现有研究的空白。TextVQA数据集为研究提供了丰富的资源，而LoRRA通过模块化设计和复制机制显著提升了性能。尽管与人类表现和理论上界仍有差距，但该工作为VQA在视障辅助等实际应用中的发展奠定了基础，并为未来研究指明了方向。

# 《MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI》

由多个作者合作完成，发表于2024年。本文详细介绍了一个新的多模态评估基准——MMT-Bench，旨在全面评估大型视觉-语言模型（Large Vision-Language Models, LVLMs）在多任务通用人工智能（AGI）方向上的能力。

### 1. 背景与研究动机


#### 1.1 大型视觉-语言模型（LVLMs）的发展


近年来，大型视觉-语言模型（LVLMs）在多模态任务（如视觉对话、视频分析、文档理解等）中展现出显著的进步。这些模型通过结合视觉和语言处理能力，能够处理复杂的多模态任务，例如视觉识别、导航和推理。文章指出，LVLMs的进步得益于多样化的高质量指令微调数据，这些数据覆盖了多个领域，推动了模型向多任务通用人工智能（AGI）的发展。


#### 1.2 现有评估基准的局限性


尽管LVLMs取得了显著进展，但当前的评估基准（如LVLM-eHub、MMBench、MME、SEED-Bench等）存在以下局限性：


- **任务覆盖范围有限**：现有基准主要测试基础能力（如视觉识别、OCR），无法全面覆盖多任务AGI所需的广泛任务类型。
- **任务复杂度不足**：现有基准的任务设计较为简单，难以评估LVLMs在需要专家知识或复杂推理的任务上的表现。
- **任务广度不足**：如MathVista和MMMU等基准专注于特定领域（如科学图表理解），缺乏对多模态任务的全面覆盖。
- **模型表现饱和**：一些LVLMs在现有基准上已接近满分（如InternLM-XComposer2在MMBench上达到79.6/100），无法有效区分模型的优劣。

因此，文章提出需要一个更全面、更具挑战性的多模态评估基准，以追踪LVLMs向多任务AGI发展的进程。



### 2. MMT-Bench 的设计与特点


#### 2.1 MMT-Bench 概述


MMT-Bench 是一个为评估 LVLMs 的多模态多任务理解能力而设计的综合性基准。它包含以下核心特点：


- **大规模任务覆盖**：MMT-Bench 包含31,325个精心设计的多选视觉问题，涵盖32个核心元任务（meta-tasks）和162个子任务（subtasks），任务数量是现有基准MMBench的8.1倍。
- **多样化的图像类型**：支持13种图像类型，包括自然场景、合成图像、深度图、文本密集图像、绘画、屏幕截图、点云、医学图像等，测试模型对不同视觉输入的适应能力。
- **多模态场景与能力**：覆盖多种多模态场景（如车辆驾驶、GUI导航、具身AI），测试14种能力，包括视觉识别、定位、推理、OCR、计数、3D感知、时间序列理解等。
- **任务地图（Task Map）**：通过任务地图分析，MMT-Bench能够帮助发现模型在“领域内”（in-domain）和“领域外”（out-of-domain）任务上的表现差异，为模型优化提供指导。

#### 2.2 任务层次结构


MMT-Bench 采用层次化的任务结构：


- **元任务（Meta-tasks）**：通过头脑风暴和去重筛选，确定了32个核心元任务，覆盖多模态理解的关键领域。
- **子任务（Subtasks）**：每个元任务进一步分解为若干子任务，总计162个。子任务的选择基于三个标准：

是否检验基本多模态能力；
是否对当前LVLMs构成挑战；
测试样本是否公开可获取。

#### 2.3 数据收集流程


MMT-Bench 的数据收集遵循高效的流水线（pipeline），包括以下步骤：


1. **数据集搜索**：基于子任务名称，通过Google、Paper With Code、Kaggle和ChatGPT等平台搜索相关数据集，并评估其适用性。
2. **元数据构建**：将数据集整理为统一的元数据格式，包含图像和元信息（如任务所需能力、视觉提示类型等）。
3. **问题与答案生成**：为每个子任务生成多选问题（最多8个选项），通过手动设计规则或使用ChatGPT生成高质量的问题和答案。例如，在“草图到图像检索”任务中，使用对应图像作为正确答案，随机抽样其他图像作为错误选项；在视频描述任务中，使用ChatGPT生成具有迷惑性的错误选项。

#### 2.4 数据统计


- **样本数量**：31,325个多选问题。
- **图像类型**：13种，包括自然场景、医学图像、点云等。
- **任务范围**：32个元任务，162个子任务，覆盖14种多模态能力。
- **场景多样性**：包括车辆驾驶、GUI导航、文档理解等实际应用场景。


### 3. 实验设计与结果


#### 3.1 实验设置


- **评估模型**：测试了30个LVLMs，包括开源模型（如InternVL-Chat、LLaVA、BLIP2）和闭源模型（如GPT-4V、GeminiProVision）。这些模型在参数规模、视觉编码器（如InternVL、CLIP-ViT）和语言模型（如QWen、LLaMA）上有所不同。
- **评估方法**：采用多选题格式，遵循OpenCompass协议提取模型的选择，优先检查选项字母（如A/B），然后检查选项内容（如“狗”/“猫”），最后使用ChatGPT提取答案。准确率（Accuracy）是主要评估指标。

#### 3.2 总体评估结果


文章对30个LVLMs在MMT-Bench上的表现进行了全面评估，结果显示：


1. **挑战性**：MMT-Bench对现有LVLMs构成显著挑战：

- 顶级模型如InternVL-Chat（63.4%准确率）、GPT-4V（62.0%）和GeminiProVision（61.6%）的整体表现远未达到理想水平。
- 去除视觉识别任务后，GeminiProVision的准确率下降至55.1%，表明模型在复杂任务上的不足。

2. **开源与闭源模型对比**：

- 开源模型InternVL-Chat（34B）表现突出，超越了闭源模型如GPT-4V和GeminiProVision，显示出通过模型规模扩展、优化训练和使用高质量数据，开源模型可媲美甚至超越闭源模型。

3. **模型规模与性能**：

- 模型规模增大（如LLaVA-v1.5从7B到13B）显著提升性能。
- 升级语言模型（如从InternLM到InternLM2）也提升了LLaVA的性能。

4. **任务表现差异**：

- LVLMs在视觉识别（VR）和视觉描述（VC）任务上表现较好，这些属于“领域内”任务。
- 在定位、像素级感知和复杂推理任务（如图像评估判断）上表现较差，这些属于“领域外”任务。

5. **指令微调的影响**：

- 未经过指令微调的BLIP2在某些任务上优于经过大量指令微调的LLaVA模型，表明指令微调可能在某些任务上损害模型的泛化能力。

#### 3.3 特定任务与提示方法分析


- **多图像 vs 单图像提示**：在多图像任务（如图像匹配、差异检测）中，使用多图像提示通常提升性能，但在单图像任务中效果有限。
- **视觉提示的影响**：某些任务（如多图像任务、坐标相关任务、视觉参照提示任务）在特定提示方法下表现更好，但大多数模型对视觉提示的利用不足，提示未来改进方向。

#### 3.4 错误分析


对顶级模型（InternVL-Chat、GPT-4V、GeminiProVision）的错误分析显示：


- 主要错误类型包括感知错误、推理错误和知识错误。
- 模型在定位和像素级感知任务上的表现较弱，反映了这些任务的挑战性。

#### 3.5 具体任务表现


文章提供了详细的子任务结果（见表A30、A32、A34、A35、A36、A37），以下是一些关键任务的表现：


- **迷因理解、计数与幻觉（表A30）**：

InternVL-Chat在迷因理解（63.0%）和计数（70.0%）上表现较好，但在幻觉检测（51.5%）上仍有改进空间。
GPT-4V在迷因理解（65.5%）和计数（75.0%）上表现优异。

- **异常检测与关键点检测（表A32）**：

QWen-VL-Plus在异常检测（80.6%）上表现最佳，但关键点检测（25.0%）普遍较弱。

- **像素级感知与多图像分析（表A34）**：

像素级感知任务（如深度估计、像素定位）是模型的弱项，最高准确率仅为69.5%（InternVL-Chat在像素识别上）。
多图像分析任务（如差异检测）表现较好，InternVL-Chat达到97.0%。

- **3D与医学理解（表A36）**：

医学模态识别任务表现较好（如GPT-4V达到99.0%），但疾病诊断和病变分级任务仍具挑战性。

- **跨图像匹配与视觉摘要（表A36、A37）**：

图像和视频描述任务表现优异（如InternVL-Chat在图像描述上达99.0%），但密集描述任务（如图像密集描述）准确率较低。


### 4. 贡献与意义


文章的贡献可以总结为以下三点：


1. **构建 MMT-Bench**：MMT-Bench 是一个全面的多模态评估基准，包含31,325个样本、32个元任务和162个子任务，覆盖多种图像类型和多模态能力，满足多任务AGI评估的任务广度要求。
2. **全面评估 LVLMs**：通过对30个LVLMs的测试，揭示了当前模型在多任务智能上的不足，特别是在定位、像素级感知和复杂推理任务上。
3. **任务地图分析**：通过任务地图，分析了模型在领域内和领域外任务上的表现差异，为未来模型优化和数据扩展提供了指导。


### 5. 未来展望


- **推动LVLMs发展**：MMT-Bench 的开源（[https://github.com/OpenGVLab/MMT-Bench）将激励社区开发更强大的多模态基础模型，接近通用多模态智能的目标。](https://github.com/OpenGVLab/MMT-Bench%EF%BC%89%E5%B0%86%E6%BF%80%E5%8A%B1%E7%A4%BE%E5%8C%BA%E5%BC%80%E5%8F%91%E6%9B%B4%E5%BC%BA%E5%A4%A7%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E6%8E%A5%E8%BF%91%E9%80%9A%E7%94%A8%E5%A4%9A%E6%A8%A1%E6%80%81%E6%99%BA%E8%83%BD%E7%9A%84%E7%9B%AE%E6%A0%87%E3%80%82)
- **改进提示方法**：研究表明，优化提示方法（如多图像提示、视觉参照提示）可提升特定任务表现，未来可进一步探索提示设计的潜力。
- **扩展数据集**：基于MMT-Bench的任务分类，可以指导监督微调数据集的扩展，改善模型在领域外任务上的表现。


### 6. 总结


《MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI》提出了一种新的评估基准，填补了现有基准在任务广度和复杂性上的不足。MMT-Bench通过大规模、多样化的任务设计，全面测试了LVLMs的多模态能力，揭示了模型在复杂任务上的局限性，并为未来的研究提供了宝贵洞见。其开放源码和任务地图分析进一步增强了其对学术界和工业界的价值，有望推动多模态通用人工智能的发展。
